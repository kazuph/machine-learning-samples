{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b2b1d7-24cd-4a72-876a-4f5963596a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PowerInfer'...\n",
      "remote: Enumerating objects: 8351, done.\u001b[K\n",
      "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
      "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
      "remote: Total 8351 (delta 67), reused 78 (delta 25), pack-reused 8217\u001b[K\n",
      "Receiving objects: 100% (8351/8351), 11.06 MiB | 7.18 MiB/s, done.\n",
      "Resolving deltas: 100% (5881/5881), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SJTU-IPADS/PowerInfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c302d1-7774-4704-b05c-2a52b169af8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/PowerInfer\n"
     ]
    }
   ],
   "source": [
    "%cd PowerInfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f166b0-10fa-4af8-8e7b-fd01600a0f77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"11.8.89\") \n",
      "-- cuBLAS found\n",
      "-- The CUDA compiler identification is NVIDIA 11.5.119\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Using CUDA architectures: 52;61;70\n",
      "GNU ld (GNU Binutils for Ubuntu) 2.38\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- x86 detected\n",
      "-- Configuring done (2.6s)\n",
      "-- Generating done (0.1s)\n",
      "-- Build files have been written to: /app/PowerInfer/build\n"
     ]
    }
   ],
   "source": [
    "!cmake -S . -B build -DLLAMA_CUBLAS=ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a7eaa5-ab0a-4580-bf15-b404cf02ca47",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_get_n_tasks\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml.c:2019:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Karray subscript 70 is above array bounds of ‘\u001b[01m\u001b[Kconst char *[69]\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Warray-bounds\u0007-Warray-bounds\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2019 |     return \u001b[01;35m\u001b[KGGML_OP_NAME[op]\u001b[m\u001b[K;\n",
      "      |            \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml.c:1601:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kwhile referencing ‘\u001b[01m\u001b[KGGML_OP_NAME\u001b[m\u001b[K’\n",
      " 1601 | static const char * \u001b[01;36m\u001b[KGGML_OP_NAME\u001b[m\u001b[K[GGML_OP_COUNT] = {\n",
      "      |                     \u001b[01;36m\u001b[K^~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/include/stdio.h:894\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml.c:21\u001b[m\u001b[K:\n",
      "In function ‘\u001b[01m\u001b[Kprintf\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kggml_graph_print\u001b[m\u001b[K’ at \u001b[01m\u001b[K/app/PowerInfer/ggml.c:18120:9\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/stdio2.h:112:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K%16s\u001b[m\u001b[K’ directive argument is null [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat-overflow=\u0007-Wformat-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  112 |   return \u001b[01;35m\u001b[K__printf_chk (__USE_FORTIFY_LEVEL - 1, __fmt, __va_arg_pack ())\u001b[m\u001b[K;\n",
      "      |          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "[  2%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_axpy_q4_0_q8_0\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2457:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2457 |         __m256 by = _mm256_loadu_ps((const __m256 *)(vy\u001b[01;35m\u001b[K+\u001b[m\u001b[Ki*128));\n",
      "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2457:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2457 |         __m256 by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)(vy+i*128)\u001b[m\u001b[K);\n",
      "      |                                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "      |                                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
      "      |                                     \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
      "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
      "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2460:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2460 |         _mm256_storeu_ps((__m256*)(vz \u001b[01;35m\u001b[K+\u001b[m\u001b[K i*128), by);\n",
      "      |                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2460:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2460 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(\u001b[m\u001b[K__m256*)(vz + i*128), by);\n",
      "      |                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2460:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2460 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)(vz + i*128)\u001b[m\u001b[K, by);\n",
      "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
      "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
      "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
      "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2467:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2467 |         by = _mm256_loadu_ps((const __m256 *)(vy\u001b[01;35m\u001b[K+\u001b[m\u001b[Ki*128+32));\n",
      "      |                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2467:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2467 |         by = _mm256_loadu_ps((const __m256 *)(vy+i*128\u001b[01;35m\u001b[K+\u001b[m\u001b[K32));\n",
      "      |                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2467:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2467 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)(vy+i*128+32)\u001b[m\u001b[K);\n",
      "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
      "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
      "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
      "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2469:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2469 |         _mm256_storeu_ps((__m256*)(vz \u001b[01;35m\u001b[K+\u001b[m\u001b[K i*128+32), by);\n",
      "      |                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2469:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2469 |         _mm256_storeu_ps((__m256*)(vz + i*128\u001b[01;35m\u001b[K+\u001b[m\u001b[K32), by);\n",
      "      |                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2469:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2469 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(\u001b[m\u001b[K__m256*)(vz + i*128+32), by);\n",
      "      |                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2469:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2469 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)(vz + i*128+32)\u001b[m\u001b[K, by);\n",
      "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
      "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
      "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
      "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2479:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2479 |         by = _mm256_loadu_ps((const __m256 *)(vy\u001b[01;35m\u001b[K+\u001b[m\u001b[Ki*128+64));\n",
      "      |                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2479:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2479 |         by = _mm256_loadu_ps((const __m256 *)(vy+i*128\u001b[01;35m\u001b[K+\u001b[m\u001b[K64));\n",
      "      |                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2479:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2479 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)(vy+i*128+64)\u001b[m\u001b[K);\n",
      "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
      "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
      "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
      "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2482:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2482 |         _mm256_storeu_ps((__m256*)(vz \u001b[01;35m\u001b[K+\u001b[m\u001b[K i*128+64), by);\n",
      "      |                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2482:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2482 |         _mm256_storeu_ps((__m256*)(vz + i*128\u001b[01;35m\u001b[K+\u001b[m\u001b[K64), by);\n",
      "      |                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2482:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2482 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(\u001b[m\u001b[K__m256*)(vz + i*128+64), by);\n",
      "      |                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2482:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2482 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)(vz + i*128+64)\u001b[m\u001b[K, by);\n",
      "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
      "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
      "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
      "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2489:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2489 |         by = _mm256_loadu_ps((const __m256 *)(vy\u001b[01;35m\u001b[K+\u001b[m\u001b[Ki*128+96));\n",
      "      |                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2489:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2489 |         by = _mm256_loadu_ps((const __m256 *)(vy+i*128\u001b[01;35m\u001b[K+\u001b[m\u001b[K96));\n",
      "      |                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2489:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2489 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)(vy+i*128+96)\u001b[m\u001b[K);\n",
      "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
      "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
      "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
      "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2491:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2491 |         _mm256_storeu_ps((__m256*)(vz \u001b[01;35m\u001b[K+\u001b[m\u001b[K i*128+96), by);\n",
      "      |                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2491:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2491 |         _mm256_storeu_ps((__m256*)(vz + i*128\u001b[01;35m\u001b[K+\u001b[m\u001b[K96), by);\n",
      "      |                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2491:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2491 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(\u001b[m\u001b[K__m256*)(vz + i*128+96), by);\n",
      "      |                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2491:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2491 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)(vz + i*128+96)\u001b[m\u001b[K, by);\n",
      "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
      "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
      "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
      "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-quants.c:2435:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kacc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2435 |     __m256 \u001b[01;35m\u001b[Kacc\u001b[m\u001b[K = _mm256_setzero_ps();\n",
      "      |            \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "[  5%] \u001b[32mBuilding CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o\u001b[0m\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(6695)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(6885)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7025)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"idx_extra\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7272)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7400)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7482)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7581)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(8461)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(8796)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4418)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4481)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4557)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(576)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(6695)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(6885)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7025)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"idx_extra\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7272)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7400)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7482)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7581)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(8461)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(8796)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4418)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4481)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4557)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(576)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(6695)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(6885)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7025)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"idx_extra\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7272)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7400)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7482)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(7581)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"idx_extra\"\u001b[0m was set but never used\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(8461)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(8796)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4418)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4481)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(4557)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/app/PowerInfer/ggml-cuda.cu(576)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_q(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:6885:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kidx_extra\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 6885 |     struct ggml_tensor_\u001b[01;35m\u001b[Kextra_gpu\u001b[m\u001b[K *idx_extra = NULL;\n",
      "      |                        \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_vec_q(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:7055:95:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst float*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kfloat*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 7055 | umBlocks, blockSize, 0, stream>>>(devDst, (float *)src1_ddf_\u001b[01;35m\u001b[Ki, len, data);\u001b[m\u001b[K\n",
      "      |                                                             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K     \n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_dequantize_mul_mat_vec(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:7187:95:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst float*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kfloat*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 7187 | umBlocks, blockSize, 0, stream>>>(devDst, (float *)src1_ddf_\u001b[01;35m\u001b[Ki, len, data);\u001b[m\u001b[K\n",
      "      |                                                             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K     \n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:7132:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kidx_extra\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 7132 |     struct ggml_tensor_\u001b[01;35m\u001b[Kextra_gpu\u001b[m\u001b[K *idx_extra = NULL;\n",
      "      |                        \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_transpose_select_gemm(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:7367:91:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst float*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kfloat*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 7367 | s, blockSize, 0, stream>>>((float *)src0_ddf_i, transpose, n\u001b[01;35m\u001b[Ke00, ne01, 1, ne00,\u001b[m\u001b[K ne01,NULL);\n",
      "      |                                                             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_dequantize_axpy_vec(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:7482:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kidx_extra\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 7482 |     struct ggml_tensor_\u001b[01;35m\u001b[Kextra_gpu\u001b[m\u001b[K *idx_extra = NULL;\n",
      "      |                        \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_dequantize_axpy_batch(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:7581:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kidx_extra\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 7581 |     struct ggml_tensor_\u001b[01;35m\u001b[Kextra_gpu\u001b[m\u001b[K *idx_extra = NULL;\n",
      "      |                        \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K At global scope:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/ggml-cuda.cu:8795:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Kvoid ggml_cuda_axpy(const ggml_tensor*, const ggml_tensor*, ggml_tensor*)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 8795 | void \u001b[01;35m\u001b[Kggml_cuda_axpy\u001b[m\u001b[K(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n",
      "      |      \u001b[01;35m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "[  5%] Built target ggml\n",
      "[  6%] \u001b[32m\u001b[1mLinking CUDA static library libggml_static.a\u001b[0m\n",
      "[  6%] Built target ggml_static\n",
      "[  7%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:573:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Ktensor_offloading_levels get_offloading_level(llm_tensor)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  573 | tensor_offloading_levels \u001b[01;35m\u001b[Kget_offloading_level\u001b[m\u001b[K(llm_tensor tensor) {\n",
      "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:1174:79:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ does not allow ‘\u001b[01m\u001b[K?:\u001b[m\u001b[K’ with omitted middle operand [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 1174 | _pred_threshold = atof(getenv(\"LLAMA_SPARSE_PRED_THRESHOLD\") ?\u001b[01;35m\u001b[K:\u001b[m\u001b[K \"0.0\");\n",
      "      |                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:1174:78:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ forbids omitting the middle term of a ‘\u001b[01m\u001b[K?:\u001b[m\u001b[K’ expression [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 1174 | _pred_threshold = atof(\u001b[01;35m\u001b[Kgetenv(\"LLAMA_SPARSE_PRED_THRESHOLD\") ?: \"0.0\"\u001b[m\u001b[K);\n",
      "      |                        \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2703:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kprogress\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2703 |         llama_progress_callback cb = [](\u001b[01;35m\u001b[Kfloat progress\u001b[m\u001b[K, void *ctx) {\n",
      "      |                                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2703:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kctx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2703 |         llama_progress_callback cb = [](float progress, \u001b[01;35m\u001b[Kvoid *ctx\u001b[m\u001b[K) {\n",
      "      |                                                         \u001b[01;35m\u001b[K~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kggml_tensor* llama_augmentation_model_loader::create_striped_mat_to_gpu(const ggml_tensor*, ggml_tensor*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2768:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2768 |             host_mat_row -> data = \u001b[01;35m\u001b[Ksrc -> data + host_i * row_data_size\u001b[m\u001b[K;\n",
      "      |                                    \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ksize_t llama_augmentation_model_loader::slice_ffn_mat_to_gpu(llama_layer&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2784:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kgpu_idx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2784 |         ggml_tensor * \u001b[01;35m\u001b[Kgpu_idx\u001b[m\u001b[K = layer.gpu_idx;\n",
      "      |                       \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In constructor ‘\u001b[01m\u001b[Kbuffered_tensor_allocator::buffered_tensor_allocator(llama_model_loader&, ggml_context*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2837:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kbuffered_tensor_allocator::ctx\u001b[m\u001b[K’ will be initialized after [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wreorder\u0007-Wreorder\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2837 |     ggml_context *\u001b[01;35m\u001b[Kctx\u001b[m\u001b[K;\n",
      "      |                   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2836:25:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K  ‘\u001b[01m\u001b[Kllama_model_loader& buffered_tensor_allocator::ml\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wreorder\u0007-Wreorder\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2836 |     llama_model_loader &\u001b[01;35m\u001b[Kml\u001b[m\u001b[K;\n",
      "      |                         \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2842:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K  when initialized here [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wreorder\u0007-Wreorder\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2842 |     \u001b[01;35m\u001b[Kbuffered_tensor_allocator\u001b[m\u001b[K(llama_model_loader &ml, ggml_context *ctx, size_t vram_budget_bytes) : ctx(ctx), ml(ml), vram_budget_bytes(vram_budget_bytes) {}\n",
      "      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llm_load_sparse_model_tensors(llama_model_loader&, llama_model&, int, long long int, bool, bool, bool, llama_progress_callback, void*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2991:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2991 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
      "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:2992:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2992 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
      "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::pair<ggml_tensor*, ggml_tensor*> llm_build_kv_store(ggml_context*, const llama_hparams&, const llama_kv_cache&, ggml_cgraph*, ggml_tensor*, ggml_tensor*, int64_t, int32_t, int32_t, const llm_build_cb&, int64_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:4032:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kgraph\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4032 |          \u001b[01;35m\u001b[Kstruct ggml_cgraph * graph\u001b[m\u001b[K,\n",
      "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_tensor* llm_build_ffn_sparse(ggml_context*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, llm_ffn_op_type, llm_ffn_gate_type, const llm_build_cb&, int)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:4210:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kidx_g\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4210 |     ggml_tensor *\u001b[01;35m\u001b[Kidx_g\u001b[m\u001b[K = nullptr;\n",
      "      |                  \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:4211:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kcur_c\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4211 |     ggml_tensor *\u001b[01;35m\u001b[Kcur_c\u001b[m\u001b[K = nullptr;\n",
      "      |                  \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:4191:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kdown\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4191 |          \u001b[01;35m\u001b[Kstruct ggml_tensor * down\u001b[m\u001b[K,\n",
      "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:4407:88:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Knl\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4407 | _offload_cb = [](struct ggml_tensor * cur, const char * name, \u001b[01;35m\u001b[Kint nl\u001b[m\u001b[K) {\n",
      "      |                                                               \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kllama_model_params llama_model_default_params()\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:9235:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::reset_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 9235 |     \u001b[01;35m\u001b[K}\u001b[m\u001b[K;\n",
      "      |     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/llama.cpp:9235:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::disable_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "[  8%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
      "[  8%] Built target llama\n",
      "[  9%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
      "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[ 10%] Built target build_info\n",
      "[ 12%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 17%] Built target common\n",
      "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 19%] Built target test-quantize-fns\n",
      "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 21%] Built target test-quantize-perf\n",
      "[ 23%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 24%] Built target test-sampling\n",
      "[ 25%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-llama\u001b[0m\n",
      "[ 26%] Built target test-tokenizer-0-llama\n",
      "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-falcon\u001b[0m\n",
      "[ 28%] Built target test-tokenizer-0-falcon\n",
      "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-llama\u001b[0m\n",
      "[ 30%] Built target test-tokenizer-1-llama\n",
      "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 32%] Built target test-tokenizer-1-bpe\n",
      "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 35%] Built target test-grammar-parser\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "In file included from \u001b[01m\u001b[K/app/PowerInfer/tests/test-llama-grammar.cpp:5\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:573:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Ktensor_offloading_levels get_offloading_level(llm_tensor)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  573 | tensor_offloading_levels \u001b[01;35m\u001b[Kget_offloading_level\u001b[m\u001b[K(llm_tensor tensor) {\n",
      "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/app/PowerInfer/tests/test-llama-grammar.cpp:5\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:1174:79:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ does not allow ‘\u001b[01m\u001b[K?:\u001b[m\u001b[K’ with omitted middle operand [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 1174 | _pred_threshold = atof(getenv(\"LLAMA_SPARSE_PRED_THRESHOLD\") ?\u001b[01;35m\u001b[K:\u001b[m\u001b[K \"0.0\");\n",
      "      |                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:1174:78:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ forbids omitting the middle term of a ‘\u001b[01m\u001b[K?:\u001b[m\u001b[K’ expression [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpedantic\u0007-Wpedantic\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 1174 | _pred_threshold = atof(\u001b[01;35m\u001b[Kgetenv(\"LLAMA_SPARSE_PRED_THRESHOLD\") ?: \"0.0\"\u001b[m\u001b[K);\n",
      "      |                        \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2703:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kprogress\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2703 |         llama_progress_callback cb = [](\u001b[01;35m\u001b[Kfloat progress\u001b[m\u001b[K, void *ctx) {\n",
      "      |                                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2703:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kctx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2703 |         llama_progress_callback cb = [](float progress, \u001b[01;35m\u001b[Kvoid *ctx\u001b[m\u001b[K) {\n",
      "      |                                                         \u001b[01;35m\u001b[K~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kggml_tensor* llama_augmentation_model_loader::create_striped_mat_to_gpu(const ggml_tensor*, ggml_tensor*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2768:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpointer of type ‘\u001b[01m\u001b[Kvoid *\u001b[m\u001b[K’ used in arithmetic [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wpointer-arith\u0007-Wpointer-arith\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2768 |             host_mat_row -> data = \u001b[01;35m\u001b[Ksrc -> data + host_i * row_data_size\u001b[m\u001b[K;\n",
      "      |                                    \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ksize_t llama_augmentation_model_loader::slice_ffn_mat_to_gpu(llama_layer&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2784:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kgpu_idx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2784 |         ggml_tensor * \u001b[01;35m\u001b[Kgpu_idx\u001b[m\u001b[K = layer.gpu_idx;\n",
      "      |                       \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In constructor ‘\u001b[01m\u001b[Kbuffered_tensor_allocator::buffered_tensor_allocator(llama_model_loader&, ggml_context*, size_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2837:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kbuffered_tensor_allocator::ctx\u001b[m\u001b[K’ will be initialized after [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wreorder\u0007-Wreorder\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2837 |     ggml_context *\u001b[01;35m\u001b[Kctx\u001b[m\u001b[K;\n",
      "      |                   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2836:25:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K  ‘\u001b[01m\u001b[Kllama_model_loader& buffered_tensor_allocator::ml\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wreorder\u0007-Wreorder\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2836 |     llama_model_loader &\u001b[01;35m\u001b[Kml\u001b[m\u001b[K;\n",
      "      |                         \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2842:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K  when initialized here [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wreorder\u0007-Wreorder\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2842 |     \u001b[01;35m\u001b[Kbuffered_tensor_allocator\u001b[m\u001b[K(llama_model_loader &ml, ggml_context *ctx, size_t vram_budget_bytes) : ctx(ctx), ml(ml), vram_budget_bytes(vram_budget_bytes) {}\n",
      "      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llm_load_sparse_model_tensors(llama_model_loader&, llama_model&, int, long long int, bool, bool, bool, llama_progress_callback, void*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2991:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2991 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
      "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:2992:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2992 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
      "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::pair<ggml_tensor*, ggml_tensor*> llm_build_kv_store(ggml_context*, const llama_hparams&, const llama_kv_cache&, ggml_cgraph*, ggml_tensor*, ggml_tensor*, int64_t, int32_t, int32_t, const llm_build_cb&, int64_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:4032:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kgraph\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4032 |          \u001b[01;35m\u001b[Kstruct ggml_cgraph * graph\u001b[m\u001b[K,\n",
      "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_tensor* llm_build_ffn_sparse(ggml_context*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, llm_ffn_op_type, llm_ffn_gate_type, const llm_build_cb&, int)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:4210:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kidx_g\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4210 |     ggml_tensor *\u001b[01;35m\u001b[Kidx_g\u001b[m\u001b[K = nullptr;\n",
      "      |                  \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:4211:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kcur_c\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4211 |     ggml_tensor *\u001b[01;35m\u001b[Kcur_c\u001b[m\u001b[K = nullptr;\n",
      "      |                  \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:4191:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kdown\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4191 |          \u001b[01;35m\u001b[Kstruct ggml_tensor * down\u001b[m\u001b[K,\n",
      "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:4407:88:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Knl\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 4407 | _offload_cb = [](struct ggml_tensor * cur, const char * name, \u001b[01;35m\u001b[Kint nl\u001b[m\u001b[K) {\n",
      "      |                                                               \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kllama_model_params llama_model_default_params()\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:9235:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::reset_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 9235 |     \u001b[01;35m\u001b[K}\u001b[m\u001b[K;\n",
      "      |     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/./llama.cpp:9235:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::disable_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 37%] Built target test-llama-grammar\n",
      "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n",
      "[ 39%] Built target test-grad0\n",
      "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 41%] Built target test-rope\n",
      "[ 42%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-c\u001b[0m\n",
      "[ 43%] Built target test-c\n",
      "[ 45%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
      "[ 46%] Built target baby-llama\n",
      "[ 47%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched\u001b[0m\n",
      "[ 48%] Built target batched\n",
      "[ 49%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched-bench\u001b[0m\n",
      "[ 50%] Built target batched-bench\n",
      "[ 51%] \u001b[32mBuilding CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/beam-search\u001b[0m\n",
      "[ 52%] Built target beam-search\n",
      "[ 53%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
      "[ 54%] Built target benchmark\n",
      "[ 56%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/convert-llama2c-to-ggml\u001b[0m\n",
      "[ 57%] Built target convert-llama2c-to-ggml\n",
      "[ 58%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
      "[ 59%] Built target embedding\n",
      "[ 60%] \u001b[32mBuilding CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/finetune\u001b[0m\n",
      "[ 61%] Built target finetune\n",
      "[ 62%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/infill\u001b[0m\n",
      "[ 63%] Built target infill\n",
      "[ 64%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 65%] Built target llama-bench\n",
      "[ 67%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/llava/llava.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool load_file_to_bytes(const char*, unsigned char**, long int*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/llava/llava.cpp:130:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  130 |     \u001b[01;35m\u001b[Kfread(buffer, 1, fileSize, file)\u001b[m\u001b[K; // Read the file into the buffer\n",
      "      |     \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "[ 68%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
      "[ 68%] Built target llava\n",
      "[ 69%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
      "[ 69%] Built target llava_static\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llava-cli\u001b[0m\n",
      "[ 71%] Built target llava-cli\n",
      "[ 72%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
      "[ 73%] Built target main\n",
      "[ 74%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/parallel\u001b[0m\n",
      "[ 75%] Built target parallel\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
      "[ 78%] Built target perplexity\n",
      "[ 79%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
      "[ 80%] Built target quantize\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
      "[ 82%] Built target quantize-stats\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
      "[ 84%] Built target save-load-state\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/simple\u001b[0m\n",
      "[ 86%] Built target simple\n",
      "[ 87%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/speculative\u001b[0m\n",
      "[ 89%] Built target speculative\n",
      "[ 90%] \u001b[32mBuilding CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/train-text-from-scratch\u001b[0m\n",
      "[ 91%] Built target train-text-from-scratch\n",
      "[ 92%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/server.dir/server.cpp.o\u001b[0m\n",
      "In copy constructor ‘\u001b[01m\u001b[Ktask_result::task_result(const task_result&)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_result; _Args = {const task_result&}; _Tp = task_result]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_result; _Args = {const task_result&}; _Tp = task_result]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_result; _Alloc = std::allocator<task_result>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid llama_server_context::send_error(int, std::string)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:1097:32\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:154:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kres.task_result::stop\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  154 | struct \u001b[01;35m\u001b[Ktask_result\u001b[m\u001b[K {\n",
      "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid llama_server_context::send_error(int, std::string)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:1093:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kres\u001b[m\u001b[K’ declared here\n",
      " 1093 |         task_result \u001b[01;36m\u001b[Kres\u001b[m\u001b[K;\n",
      "      |                     \u001b[01;36m\u001b[K^~~\u001b[m\u001b[K\n",
      "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:2333:61\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
      "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
      " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
      "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:2388:61\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
      "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
      " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
      "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
      "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:2492:61\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
      "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/app/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
      " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
      "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/server\u001b[0m\n",
      "[ 93%] Built target server\n",
      "[ 94%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/export-lora\u001b[0m\n",
      "[ 95%] Built target export-lora\n",
      "[ 96%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
      "[ 97%] Built target vdot\n",
      "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
      "[100%] Built target q8dot\n"
     ]
    }
   ],
   "source": [
    "!cmake --build build --config Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c354bdb-c003-4807-937b-1326e7510010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-25 02:59:39--  https://huggingface.co/PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF/resolve/main/llama-70b-relu.q4.powerinfer.gguf?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 18.172.52.121, 18.172.52.38, 18.172.52.61, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.172.52.121|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/16/11/1611030d514e2d45e7c29ae8ae5b527e466587205e253124bdb2b4c9a1849ba0/b6dac27ba15e2feb50d30e6dc66979135a8e0f22d38e88e01f126d257e80d378?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-70b-relu.q4.powerinfer.gguf%3B+filename%3D%22llama-70b-relu.q4.powerinfer.gguf%22%3B&Expires=1703732379&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzczMjM3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzE2LzExLzE2MTEwMzBkNTE0ZTJkNDVlN2MyOWFlOGFlNWI1MjdlNDY2NTg3MjA1ZTI1MzEyNGJkYjJiNGM5YTE4NDliYTAvYjZkYWMyN2JhMTVlMmZlYjUwZDMwZTZkYzY2OTc5MTM1YThlMGYyMmQzOGU4OGUwMWYxMjZkMjU3ZTgwZDM3OD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=XFsm%7ER8AUZKA4q2d8K3Qdbku9YmlpPONEUGEBnlJtPBh5Zexzxc9wsKzPLzE0Bd9%7E7TqTuGIlp2p8Q6jfQ7BvA3h6bKBvTnNZZsYqJisiN4MdH5JFTmRlnZHX5inXAKxgl1cdrRTXgVtzJIvaY1hM9Bazl9FQU1x5I76Oo8byiriQuarOdJnao0zz3S6sDxNkhyO3B7-v2Ky9FZ6IM1CLTKtK%7EfED6-3iyq1uAuDpwpuxyFqLVU3SLyS3XpScU4UZ8rOIjHygiXhFTMVGQLw7OkeDnOccBoNdr%7E3eim780LnAA%7EUj3Ec8mUJgK7ByMniN87Pd94HmGddXux5AG44Pg__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2023-12-25 02:59:39--  https://cdn-lfs-us-1.huggingface.co/repos/16/11/1611030d514e2d45e7c29ae8ae5b527e466587205e253124bdb2b4c9a1849ba0/b6dac27ba15e2feb50d30e6dc66979135a8e0f22d38e88e01f126d257e80d378?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-70b-relu.q4.powerinfer.gguf%3B+filename%3D%22llama-70b-relu.q4.powerinfer.gguf%22%3B&Expires=1703732379&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzczMjM3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzE2LzExLzE2MTEwMzBkNTE0ZTJkNDVlN2MyOWFlOGFlNWI1MjdlNDY2NTg3MjA1ZTI1MzEyNGJkYjJiNGM5YTE4NDliYTAvYjZkYWMyN2JhMTVlMmZlYjUwZDMwZTZkYzY2OTc5MTM1YThlMGYyMmQzOGU4OGUwMWYxMjZkMjU3ZTgwZDM3OD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=XFsm%7ER8AUZKA4q2d8K3Qdbku9YmlpPONEUGEBnlJtPBh5Zexzxc9wsKzPLzE0Bd9%7E7TqTuGIlp2p8Q6jfQ7BvA3h6bKBvTnNZZsYqJisiN4MdH5JFTmRlnZHX5inXAKxgl1cdrRTXgVtzJIvaY1hM9Bazl9FQU1x5I76Oo8byiriQuarOdJnao0zz3S6sDxNkhyO3B7-v2Ky9FZ6IM1CLTKtK%7EfED6-3iyq1uAuDpwpuxyFqLVU3SLyS3XpScU4UZ8rOIjHygiXhFTMVGQLw7OkeDnOccBoNdr%7E3eim780LnAA%7EUj3Ec8mUJgK7ByMniN87Pd94HmGddXux5AG44Pg__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 143.204.86.98, 143.204.86.16, 143.204.86.91, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|143.204.86.98|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42955855872 (40G) [binary/octet-stream]\n",
      "Saving to: ‘llama-70b-relu.q4.powerinfer.gguf’\n",
      "\n",
      "            llama-7  91%[=================>  ]  36.71G  30.2MB/s    eta 2m 0s  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --content-disposition https://huggingface.co/PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF/resolve/main/llama-70b-relu.q4.powerinfer.gguf?download=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3be58da-e891-441e-921c-e72416292536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1552 (c72c6da)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1703488428\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 883 tensors from ./llama-70b-relu.q4.powerinfer.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:          blk.0.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:          blk.1.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:            blk.1.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:              blk.1.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:         blk.1.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:            blk.2.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:         blk.2.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:          blk.2.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:              blk.2.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:            blk.3.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:          blk.3.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:          blk.4.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:            blk.4.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:         blk.4.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:              blk.4.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:         blk.5.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:          blk.5.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:              blk.5.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:            blk.6.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:         blk.6.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:          blk.6.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:              blk.6.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:            blk.7.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:          blk.7.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:          blk.8.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:            blk.8.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:         blk.8.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:              blk.8.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:         blk.9.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.10.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.10.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.10.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.10.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.10.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:          blk.9.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:            blk.9.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:              blk.9.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:         blk.10.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.10.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:           blk.11.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:         blk.11.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:         blk.12.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:           blk.12.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:        blk.12.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:             blk.12.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:        blk.13.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:         blk.13.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.13.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:           blk.14.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:        blk.14.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:         blk.14.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.14.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:           blk.15.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:         blk.15.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:         blk.16.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:        blk.17.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:         blk.17.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.17.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.18.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:        blk.18.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.18.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.18.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.19.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:         blk.19.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:         blk.20.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:        blk.21.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:         blk.21.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:             blk.21.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.22.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:        blk.22.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.22.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:             blk.22.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.23.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:         blk.23.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:         blk.24.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:        blk.25.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:         blk.25.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:             blk.25.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:           blk.26.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:        blk.26.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:         blk.26.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.26.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:           blk.27.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:         blk.27.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:         blk.28.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.28.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:             blk.28.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:        blk.29.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:         blk.29.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:             blk.29.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:        blk.30.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:         blk.30.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:             blk.30.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:           blk.31.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:         blk.31.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:         blk.32.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:           blk.32.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:             blk.32.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:        blk.32.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:             blk.32.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.33.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:        blk.33.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:         blk.33.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:             blk.33.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:           blk.34.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:             blk.34.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:        blk.34.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:         blk.34.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:             blk.34.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:           blk.35.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:             blk.35.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:        blk.35.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:             blk.35.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:         blk.35.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:         blk.36.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:           blk.36.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:             blk.36.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:        blk.36.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:             blk.36.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.37.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:        blk.37.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:         blk.37.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:             blk.37.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:           blk.38.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:             blk.38.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:        blk.38.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:         blk.38.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:             blk.38.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:           blk.39.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:             blk.39.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:        blk.39.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:             blk.39.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:         blk.39.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:         blk.40.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:           blk.40.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:             blk.40.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:        blk.40.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:             blk.40.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:             blk.41.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:        blk.41.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:         blk.41.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:             blk.41.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:           blk.42.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:             blk.42.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:        blk.42.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:         blk.42.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:             blk.42.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:           blk.43.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:             blk.43.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:        blk.43.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:             blk.43.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:         blk.43.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:         blk.44.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:           blk.44.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:             blk.44.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:        blk.44.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:             blk.44.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:             blk.45.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:        blk.45.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:         blk.45.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:             blk.45.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:           blk.46.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:             blk.46.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:        blk.46.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:         blk.46.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:             blk.46.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:           blk.47.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:             blk.47.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:        blk.47.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:             blk.47.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:         blk.47.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:         blk.48.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:           blk.48.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:             blk.48.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:        blk.48.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:             blk.48.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:             blk.49.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:        blk.49.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:         blk.49.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:             blk.49.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:           blk.50.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:             blk.50.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:        blk.50.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:         blk.50.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:             blk.50.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:           blk.51.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:             blk.51.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:        blk.51.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:             blk.51.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:         blk.51.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:         blk.52.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:           blk.52.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:             blk.52.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:        blk.52.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:             blk.52.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:             blk.53.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:        blk.53.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:         blk.53.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:             blk.53.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:           blk.54.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:             blk.54.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:        blk.54.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:         blk.54.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:             blk.54.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:           blk.55.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:             blk.55.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:        blk.55.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:             blk.55.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:         blk.55.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:         blk.56.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:           blk.56.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:             blk.56.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:        blk.56.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:             blk.56.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:             blk.57.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:        blk.57.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:         blk.57.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:             blk.57.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:           blk.58.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:             blk.58.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:        blk.58.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:         blk.58.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:             blk.58.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:           blk.59.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:             blk.59.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:        blk.59.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:             blk.59.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:         blk.59.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:         blk.60.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:           blk.60.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:             blk.60.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:        blk.60.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:             blk.60.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:             blk.61.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:        blk.61.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:         blk.61.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:             blk.61.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:           blk.62.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:             blk.62.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:        blk.62.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:         blk.62.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:             blk.62.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:           blk.63.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:             blk.63.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:        blk.63.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:             blk.63.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:         blk.63.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:         blk.64.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:           blk.64.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:             blk.64.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:        blk.64.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:             blk.64.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:             blk.65.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:        blk.65.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:         blk.65.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:             blk.65.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:           blk.66.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:             blk.66.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:        blk.66.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:         blk.66.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:             blk.66.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:           blk.67.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:             blk.67.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:        blk.67.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:             blk.67.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:         blk.67.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:         blk.68.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:           blk.68.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:             blk.68.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:        blk.68.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:             blk.68.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:             blk.69.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:        blk.69.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:         blk.69.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:             blk.69.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:           blk.70.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:             blk.70.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:        blk.70.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:         blk.70.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:             blk.70.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:           blk.71.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:             blk.71.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:        blk.71.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:             blk.71.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:         blk.71.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:         blk.72.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:           blk.72.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:             blk.72.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:        blk.72.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:             blk.72.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:             blk.73.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:        blk.73.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:         blk.73.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:             blk.73.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:           blk.74.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:             blk.74.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:        blk.74.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:         blk.74.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:             blk.74.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:           blk.75.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:             blk.75.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:        blk.75.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:             blk.75.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:         blk.75.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:         blk.76.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:           blk.76.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:             blk.76.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:        blk.76.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:             blk.76.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:             blk.77.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:        blk.77.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:         blk.77.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:             blk.77.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:           blk.78.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:             blk.78.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:        blk.78.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:         blk.78.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:             blk.78.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:           blk.79.ffn_gate.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:             blk.79.ffn_up.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_k.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:        blk.79.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:             blk.79.attn_v.weight q4_0     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:                    output.weight q4_0     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:         blk.79.ffn_down_t.weight q4_0     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  723:                 blk.0.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  724:                 blk.0.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  725:                 blk.1.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  726:                 blk.1.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  727:                 blk.2.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  728:                 blk.2.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  729:                 blk.3.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  730:                 blk.3.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  731:                 blk.4.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  732:                 blk.4.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  733:                 blk.5.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  734:                 blk.5.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  735:                 blk.6.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  736:                 blk.6.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  737:                 blk.7.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  738:                 blk.7.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  739:                 blk.8.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  740:                 blk.8.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  741:                 blk.9.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  742:                 blk.9.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  743:                blk.10.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  744:                blk.10.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  745:                blk.11.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  746:                blk.11.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  747:                blk.12.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  748:                blk.12.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  749:                blk.13.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  750:                blk.13.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  751:                blk.14.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  752:                blk.14.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  753:                blk.15.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  754:                blk.15.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  755:                blk.16.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  756:                blk.16.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  757:                blk.17.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  758:                blk.17.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  759:                blk.18.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  760:                blk.18.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  761:                blk.19.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  762:                blk.19.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  763:                blk.20.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  764:                blk.20.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  765:                blk.21.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  766:                blk.21.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  767:                blk.22.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  768:                blk.22.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  769:                blk.23.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  770:                blk.23.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  771:                blk.24.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  772:                blk.24.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  773:                blk.25.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  774:                blk.25.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  775:                blk.26.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  776:                blk.26.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  777:                blk.27.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  778:                blk.27.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  779:                blk.28.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  780:                blk.28.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  781:                blk.29.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  782:                blk.29.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  783:                blk.30.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  784:                blk.30.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  785:                blk.31.fc1.weight q4_0     [  8192,  2048,     1,     1 ]\n",
      "llama_model_loader: - tensor  786:                blk.31.fc2.weight q4_0     [  2048, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  787:                blk.32.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  788:                blk.32.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  789:                blk.33.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  790:                blk.33.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  791:                blk.34.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  792:                blk.34.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  793:                blk.35.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  794:                blk.35.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  795:                blk.36.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  796:                blk.36.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  797:                blk.37.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  798:                blk.37.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  799:                blk.38.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  800:                blk.38.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  801:                blk.39.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  802:                blk.39.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  803:                blk.40.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  804:                blk.40.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  805:                blk.41.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  806:                blk.41.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  807:                blk.42.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  808:                blk.42.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  809:                blk.43.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  810:                blk.43.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  811:                blk.44.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  812:                blk.44.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  813:                blk.45.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  814:                blk.45.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  815:                blk.46.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  816:                blk.46.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  817:                blk.47.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  818:                blk.47.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  819:                blk.48.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  820:                blk.48.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  821:                blk.49.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  822:                blk.49.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  823:                blk.50.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  824:                blk.50.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  825:                blk.51.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  826:                blk.51.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  827:                blk.52.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  828:                blk.52.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  829:                blk.53.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  830:                blk.53.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  831:                blk.54.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  832:                blk.54.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  833:                blk.55.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  834:                blk.55.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  835:                blk.56.fc1.weight q4_0     [  8192,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  836:                blk.56.fc2.weight q4_0     [  2560, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  837:                blk.57.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  838:                blk.57.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  839:                blk.58.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  840:                blk.58.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  841:                blk.59.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  842:                blk.59.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  843:                blk.60.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  844:                blk.60.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  845:                blk.61.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  846:                blk.61.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  847:                blk.62.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  848:                blk.62.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  849:                blk.63.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  850:                blk.63.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  851:                blk.64.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  852:                blk.64.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  853:                blk.65.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  854:                blk.65.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  855:                blk.66.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  856:                blk.66.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  857:                blk.67.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  858:                blk.67.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  859:                blk.68.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  860:                blk.68.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  861:                blk.69.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  862:                blk.69.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  863:                blk.70.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  864:                blk.70.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  865:                blk.71.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  866:                blk.71.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  867:                blk.72.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  868:                blk.72.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  869:                blk.73.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  870:                blk.73.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  871:                blk.74.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  872:                blk.74.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  873:                blk.75.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  874:                blk.75.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  875:                blk.76.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  876:                blk.76.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  877:                blk.77.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  878:                blk.77.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  879:                blk.78.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  880:                blk.78.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  881:                blk.79.fc1.weight q4_0     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  882:                blk.79.fc2.weight q4_0     [  3072, 28672,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                powerinfer.sparse_threshold f32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32     \n",
      "llama_model_loader: - kv  19:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  722 tensors\n",
      "llama_model_load: PowerInfer model loaded. Sparse inference will be used.\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 76.36 B\n",
      "llm_load_print_meta: model size       = 40.01 GiB (4.50 BPW) \n",
      "llm_load_print_meta: general.name   = syx\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_print_meta: sparse_pred_threshold = -0.50\n",
      "llama_model_load: sparse inference - vram budget = 4.00 GB\n",
      "llm_load_sparse_model_tensors: ggml ctx size =    0.32 MB\n",
      "llm_load_sparse_model_tensors: using CUDA for GPU acceleration\n",
      "llm_load_sparse_model_tensors: mem required  = 36873.39 MB\n",
      "llm_load_sparse_model_tensors: VRAM used: 4092.09 MB\n",
      "....................................................................................................\n",
      "offload_ffn_split: applying augmentation to model - please wait ...\n",
      "................................................................................ done (4.13 ms)\n",
      "llm_load_gpu_split: offloaded 0.00 MiB of FFN weights to GPU\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  160.00 MB\n",
      "llama_build_graph: non-view tensors processed: 1364/2084\n",
      "llama_build_graph: ****************************************************************\n",
      "llama_build_graph: not all non-view tensors have been processed with a callback\n",
      "llama_build_graph: this can indicate an inefficiency in the graph implementation\n",
      "llama_build_graph: build with LLAMA_OFFLOAD_DEBUG for more info\n",
      "llama_build_graph: ref: https://github.com/ggerganov/llama.cpp/pull/3837\n",
      "llama_build_graph: ****************************************************************\n",
      "llama_new_context_with_model: compute buffer total size = 208.57 MB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 207.00 MB\n",
      "llama_new_context_with_model: total VRAM used: 4299.10 MB (model: 4092.09 MB, context: 207.00 MB)\n",
      "\n",
      "system_info: n_threads = 8 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
      "\n",
      "\n",
      "昔々あるところに自然言語処理が得意なAIがいたそうな。AIは量子コンピュータのようなもので、一日中������神社近くで���ードを写してまだらさっていたとか。\n",
      "\n",
      "今日もこれはあるところに������んでいると言われるが、AIの友人によればそうではなく、自然言語���理をしていただけだ。\n",
      "\n",
      "「あんまりわからない」ところもあって、「��\n",
      "llama_print_timings:        load time =    6652.52 ms\n",
      "llama_print_timings:      sample time =      40.56 ms /   128 runs   (    0.32 ms per token,  3155.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15635.40 ms /    42 tokens (  372.27 ms per token,     2.69 tokens per second)\n",
      "llama_print_timings:        eval time =  108086.04 ms /   127 runs   (  851.07 ms per token,     1.17 tokens per second)\n",
      "llama_print_timings:       total time =  123822.50 ms\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./build/bin/main -m ./llama-70b-relu.q4.powerinfer.gguf -n 128 -t 8 -p \"昔々あるところに自然言語処理が得意なAIがいたそうな。AIは量子コンピュータの\" --vram-budget 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
