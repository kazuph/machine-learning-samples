{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b24518-f25a-4592-80fa-1426a2aabdc5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 24 06:36:51 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:08:00.0  On |                  N/A |\n",
      "| 30%   47C    P2   127W / 350W |   1467MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "fatal: destination path 'generative-models' already exists and is not an empty directory.\n",
      "Collecting clip@ git+https://github.com/openai/CLIP.git (from -r generative-models/requirements/pt2.txt (line 3))\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-install-dr69y1it/clip_a11aa92d23244f9188cad006ccdc5380\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-install-dr69y1it/clip_a11aa92d23244f9188cad006ccdc5380\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting black==23.7.0 (from -r generative-models/requirements/pt2.txt (line 1))\n",
      "  Obtaining dependency information for black==23.7.0 from https://files.pythonhosted.org/packages/e4/17/a819f00990e8cf4e652186603ddc8d29477362da2b7717858732b6abd13d/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc7d23c4550>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/e4/17/a819f00990e8cf4e652186603ddc8d29477362da2b7717858732b6abd13d/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting chardet==5.1.0 (from -r generative-models/requirements/pt2.txt (line 2))\n",
      "  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: einops>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 4)) (0.7.0)\n",
      "Collecting fairscale>=0.4.13 (from -r generative-models/requirements/pt2.txt (line 5))\n",
      "  Using cached fairscale-0.4.13-py3-none-any.whl\n",
      "Collecting fire>=0.5.0 (from -r generative-models/requirements/pt2.txt (line 6))\n",
      "  Using cached fire-0.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 7)) (2023.9.2)\n",
      "Collecting invisible-watermark>=0.2.0 (from -r generative-models/requirements/pt2.txt (line 8))\n",
      "  Obtaining dependency information for invisible-watermark>=0.2.0 from https://files.pythonhosted.org/packages/2b/57/18b5a914f6d7994dd349252873169e946dc824328e9a37fd15ed836deedc/invisible_watermark-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading invisible_watermark-0.2.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting kornia==0.6.9 (from -r generative-models/requirements/pt2.txt (line 9))\n",
      "  Downloading kornia-0.6.9-py2.py3-none-any.whl (569 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 10)) (3.8.2)\n",
      "Collecting natsort>=8.4.0 (from -r generative-models/requirements/pt2.txt (line 11))\n",
      "  Obtaining dependency information for natsort>=8.4.0 from https://files.pythonhosted.org/packages/ef/82/7a9d0550484a62c6da82858ee9419f3dd1ccc9aa1c26a1e43da3ecd20b0d/natsort-8.4.0-py3-none-any.whl.metadata\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting ninja>=1.11.1 (from -r generative-models/requirements/pt2.txt (line 12))\n",
      "  Obtaining dependency information for ninja>=1.11.1 from https://files.pythonhosted.org/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting numpy>=1.24.4 (from -r generative-models/requirements/pt2.txt (line 13))\n",
      "  Obtaining dependency information for numpy>=1.24.4 from https://files.pythonhosted.org/packages/64/41/284783f1014685201e447ea976e85fed0e351f5debbaf3ee6d7645521f1d/numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 14)) (2.3.0)\n",
      "Collecting open-clip-torch>=2.20.0 (from -r generative-models/requirements/pt2.txt (line 15))\n",
      "  Obtaining dependency information for open-clip-torch>=2.20.0 from https://files.pythonhosted.org/packages/7c/7f/952fdffa17b15d0c7c51a730860fcf4f4982528ecc753b190dcd46cc944b/open_clip_torch-2.23.0-py3-none-any.whl.metadata\n",
      "  Downloading open_clip_torch-2.23.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting opencv-python==4.6.0.66 (from -r generative-models/requirements/pt2.txt (line 16))\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 17)) (2.1.3)\n",
      "Collecting pillow>=9.5.0 (from -r generative-models/requirements/pt2.txt (line 18))\n",
      "  Obtaining dependency information for pillow>=9.5.0 from https://files.pythonhosted.org/packages/e5/b9/5c6ad3241f1ccca4b781dfeddbab2dac4480f95aedc351a0e60c9f4c8aa9/Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pudb>=2022.1.3 (from -r generative-models/requirements/pt2.txt (line 19))\n",
      "  Downloading pudb-2023.1.tar.gz (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.3/224.3 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorch-lightning==2.0.1 (from -r generative-models/requirements/pt2.txt (line 20))\n",
      "  Downloading pytorch_lightning-2.0.1-py3-none-any.whl (716 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m716.4/716.4 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 21)) (6.0.1)\n",
      "Collecting scipy>=1.10.1 (from -r generative-models/requirements/pt2.txt (line 22))\n",
      "  Obtaining dependency information for scipy>=1.10.1 from https://files.pythonhosted.org/packages/e0/9e/80e2205d138960a49caea391f3710600895dd8292b6868dc9aff7aa593f9/scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting streamlit>=0.73.1 (from -r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for streamlit>=0.73.1 from https://files.pythonhosted.org/packages/9d/9f/09fe6469e891031596872bd50bff90d47bea5c32d426235714cf24662740/streamlit-1.28.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading streamlit-1.28.2-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting tensorboardx==2.6 (from -r generative-models/requirements/pt2.txt (line 24))\n",
      "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm>=0.9.2 (from -r generative-models/requirements/pt2.txt (line 25))\n",
      "  Obtaining dependency information for timm>=0.9.2 from https://files.pythonhosted.org/packages/76/aa/4b54f6047c442883243f68f6f9e3a0ab77aaae4b3e6e51a98b371e73dd77/timm-0.9.11-py3-none-any.whl.metadata\n",
      "  Downloading timm-0.9.11-py3-none-any.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers==0.12.1 (from -r generative-models/requirements/pt2.txt (line 26))\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 27)) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchaudio>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 28)) (2.0.2+cu118)\n",
      "Collecting torchdata==0.6.1 (from -r generative-models/requirements/pt2.txt (line 29))\n",
      "  Downloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics>=1.0.1 (from -r generative-models/requirements/pt2.txt (line 30))\n",
      "  Obtaining dependency information for torchmetrics>=1.0.1 from https://files.pythonhosted.org/packages/a3/88/cc27059747ddecff744826e38014822023cbfff4ca079a6ee9a96602dd0b/torchmetrics-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: torchvision>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 31)) (0.15.2+cu118)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 32)) (4.66.1)\n",
      "Collecting transformers==4.19.1 (from -r generative-models/requirements/pt2.txt (line 33))\n",
      "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 34)) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from -r generative-models/requirements/pt2.txt (line 35)) (1.26.13)\n",
      "Collecting wandb>=0.15.6 (from -r generative-models/requirements/pt2.txt (line 36))\n",
      "  Obtaining dependency information for wandb>=0.15.6 from https://files.pythonhosted.org/packages/5c/81/1780aa129564b11709a6d7f0739257174f0a3a1b432ba804b3c6f00e0f88/wandb-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting webdataset>=0.2.33 (from -r generative-models/requirements/pt2.txt (line 37))\n",
      "  Obtaining dependency information for webdataset>=0.2.33 from https://files.pythonhosted.org/packages/33/34/a1b11fe9eecde3726ef50783317afd220ca0958117cedac3ae9c3c227878/webdataset-0.2.79-py3-none-any.whl.metadata\n",
      "  Downloading webdataset-0.2.79-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting wheel>=0.41.0 (from -r generative-models/requirements/pt2.txt (line 38))\n",
      "  Obtaining dependency information for wheel>=0.41.0 from https://files.pythonhosted.org/packages/fa/7f/4c07234086edbce4a0a446209dc0cb08a19bb206a3ea53b2f56a403f983b/wheel-0.41.3-py3-none-any.whl.metadata\n",
      "  Downloading wheel-0.41.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting xformers>=0.0.20 (from -r generative-models/requirements/pt2.txt (line 39))\n",
      "  Obtaining dependency information for xformers>=0.0.20 from https://files.pythonhosted.org/packages/bb/9c/bb1a40d5e8db2b34dd6e6c0f851e86b38a3d0840fff1bf14240eff7d3da6/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r generative-models/requirements/pt2.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r generative-models/requirements/pt2.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r generative-models/requirements/pt2.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r generative-models/requirements/pt2.txt (line 1)) (0.11.2)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r generative-models/requirements/pt2.txt (line 1)) (3.10.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r generative-models/requirements/pt2.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==2.0.1->-r generative-models/requirements/pt2.txt (line 20)) (4.8.0)\n",
      "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning==2.0.1->-r generative-models/requirements/pt2.txt (line 20))\n",
      "  Obtaining dependency information for lightning-utilities>=0.7.0 from https://files.pythonhosted.org/packages/5e/f4/07b748cb9834848de16aaeb1ae38bc9cfcfe3adc22ee2c8ebbe11db82795/lightning_utilities-0.10.0-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting protobuf<4,>=3.8.0 (from tensorboardx==2.6->-r generative-models/requirements/pt2.txt (line 24))\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->-r generative-models/requirements/pt2.txt (line 29)) (2.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r generative-models/requirements/pt2.txt (line 27)) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r generative-models/requirements/pt2.txt (line 27)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r generative-models/requirements/pt2.txt (line 27)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r generative-models/requirements/pt2.txt (line 27)) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r generative-models/requirements/pt2.txt (line 33)) (0.17.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r generative-models/requirements/pt2.txt (line 33)) (2023.10.3)\n",
      "Collecting cmake (from triton==2.0.0->-r generative-models/requirements/pt2.txt (line 34))\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/5a/e1/001da8b79b5d336d42aee95aae4cb934348ffa8925a6280fcd81859d8734/cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->-r generative-models/requirements/pt2.txt (line 34)) (15.0.7)\n",
      "Collecting ftfy (from clip@ git+https://github.com/openai/CLIP.git->-r generative-models/requirements/pt2.txt (line 3))\n",
      "  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/91/f8/dfa32d06cfcbdb76bc46e0f5d69c537de33f4cedb1a15cd4746ab45a6a26/ftfy-6.1.3-py3-none-any.whl.metadata\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.5.0->-r generative-models/requirements/pt2.txt (line 6)) (1.16.0)\n",
      "Collecting termcolor (from fire>=0.5.0->-r generative-models/requirements/pt2.txt (line 6))\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from invisible-watermark>=0.2.0->-r generative-models/requirements/pt2.txt (line 8))\n",
      "  Obtaining dependency information for PyWavelets>=1.1.1 from https://files.pythonhosted.org/packages/b9/c2/7dffd331ac88c1776d8bba0fa77c73202a4ce79dc8d203fc99007b715866/pywavelets-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pywavelets-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r generative-models/requirements/pt2.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r generative-models/requirements/pt2.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r generative-models/requirements/pt2.txt (line 10)) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r generative-models/requirements/pt2.txt (line 10)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r generative-models/requirements/pt2.txt (line 10)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r generative-models/requirements/pt2.txt (line 10)) (2.8.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.3.0->-r generative-models/requirements/pt2.txt (line 14)) (4.9.3)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.20.0->-r generative-models/requirements/pt2.txt (line 15)) (0.1.99)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->-r generative-models/requirements/pt2.txt (line 17)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->-r generative-models/requirements/pt2.txt (line 17)) (2023.3)\n",
      "Requirement already satisfied: jedi<1,>=0.18 in /usr/local/lib/python3.10/dist-packages (from pudb>=2022.1.3->-r generative-models/requirements/pt2.txt (line 19)) (0.19.0)\n",
      "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from pudb>=2022.1.3->-r generative-models/requirements/pt2.txt (line 19)) (2.16.1)\n",
      "Collecting urwid>=1.1.1 (from pudb>=2022.1.3->-r generative-models/requirements/pt2.txt (line 19))\n",
      "  Obtaining dependency information for urwid>=1.1.1 from https://files.pythonhosted.org/packages/7e/a8/ef34bc51991185ede850cfc8da1cb1ebf92cc7cd9304366af23912343ab6/urwid-2.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading urwid-2.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting urwid_readline (from pudb>=2022.1.3->-r generative-models/requirements/pt2.txt (line 19))\n",
      "  Downloading urwid_readline-0.13.tar.gz (7.9 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (5.1.2)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for blinker<2,>=1.0.0 from https://files.pythonhosted.org/packages/fa/2a/7f3714cbc6356a0efec525ce7a0613d581072ed6eb53eb7b9754f33db807/blinker-1.7.0-py3-none-any.whl.metadata\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for cachetools<6,>=4.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting importlib-metadata<7,>=1.4 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for importlib-metadata<7,>=1.4 from https://files.pythonhosted.org/packages/cc/37/db7ba97e676af155f5fcb1a35466f446eadc9104e25b83366e8088c9c926/importlib_metadata-6.8.0-py3-none-any.whl.metadata\n",
      "  Using cached importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pyarrow>=6.0 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for pyarrow>=6.0 from https://files.pythonhosted.org/packages/34/65/204f7c0d507056c37b56dddb3bd60f55744f2609c0f96a5e4ca91c67c42a/pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (13.7.0)\n",
      "Collecting tenacity<9,>=8.1.0 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for tenacity<9,>=8.1.0 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for tzlocal<6,>=1.1 from https://files.pythonhosted.org/packages/97/3f/c4c51c55ff8487f2e6d0e618dba917e3c3ee2caae6cf0fbb59c9b1876f2e/tzlocal-5.2-py3-none-any.whl.metadata\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for validators<1,>=0.2 from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (3.1.36)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (6.3.3)\n",
      "Collecting watchdog>=2.1.5 (from streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Using cached watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->-r generative-models/requirements/pt2.txt (line 25)) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.6->-r generative-models/requirements/pt2.txt (line 36)) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb>=0.15.6->-r generative-models/requirements/pt2.txt (line 36))\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/68/b1/161407567dceeb2de39ac973b2aff00439b575fdac95d8911ca3424d7323/sentry_sdk-1.36.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-1.36.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.15.6->-r generative-models/requirements/pt2.txt (line 36))\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle (from wandb>=0.15.6->-r generative-models/requirements/pt2.txt (line 36))\n",
      "  Obtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/79/e7/54b36be02aee8ad573be68f6f46fd62838735c2f007b22df50eb5e13a20d/setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb>=0.15.6->-r generative-models/requirements/pt2.txt (line 36)) (59.6.0)\n",
      "Collecting appdirs>=1.4.3 (from wandb>=0.15.6->-r generative-models/requirements/pt2.txt (line 36))\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting braceexpand (from webdataset>=0.2.33->-r generative-models/requirements/pt2.txt (line 37))\n",
      "  Using cached braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting xformers>=0.0.20 (from -r generative-models/requirements/pt2.txt (line 39))\n",
      "  Obtaining dependency information for xformers>=0.0.20 from https://files.pythonhosted.org/packages/c2/2f/0fa3c080f00c2ebf6836bb1109b9d48b03a4f446e89a058b6c08ba3f7ba1/xformers-0.0.22.post4-cp310-cp310-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xformers-0.0.22.post4-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "  Obtaining dependency information for xformers>=0.0.20 from https://files.pythonhosted.org/packages/52/ca/82aeee5dcc24a3429ff5de65cc58ae9695f90f49fbba71755e7fab69a706/xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (4.19.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (0.12.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec>=2023.6.0->-r generative-models/requirements/pt2.txt (line 7))\n",
      "  Obtaining dependency information for aiohttp!=4.0.0a0,!=4.0.0a1 from https://files.pythonhosted.org/packages/b0/36/c7bd200871e7351ab8396e8edcbb91e1198e0ded67a0824c93110c4c5df2/aiohttp-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (4.0.10)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7,>=1.4->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23))\n",
      "  Obtaining dependency information for zipp>=0.5 from https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl.metadata\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi<1,>=0.18->pudb>=2022.1.3->-r generative-models/requirements/pt2.txt (line 19)) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->-r generative-models/requirements/pt2.txt (line 27)) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.1->-r generative-models/requirements/pt2.txt (line 29)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.1->-r generative-models/requirements/pt2.txt (line 29)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.1->-r generative-models/requirements/pt2.txt (line 29)) (2022.12.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (3.0.0)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->clip@ git+https://github.com/openai/CLIP.git->-r generative-models/requirements/pt2.txt (line 3))\n",
      "  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/31/b1/a59de0ad3aabb17523a39804f4c6df3ae87ead053a4e25362ae03d73d03a/wcwidth-0.2.12-py2.py3-none-any.whl.metadata\n",
      "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->-r generative-models/requirements/pt2.txt (line 27)) (1.2.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.6.0->-r generative-models/requirements/pt2.txt (line 7)) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.6.0->-r generative-models/requirements/pt2.txt (line 7))\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.6.0->-r generative-models/requirements/pt2.txt (line 7))\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/b6/b2/44b31699e27f82c577143d062a2b58cbe0c6e7a0828d13c0ffd10891ad40/yarl-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading yarl-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.6.0->-r generative-models/requirements/pt2.txt (line 7))\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.6.0->-r generative-models/requirements/pt2.txt (line 7))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.6.0->-r generative-models/requirements/pt2.txt (line 7))\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (0.10.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=0.73.1->-r generative-models/requirements/pt2.txt (line 23)) (0.1.2)\n",
      "Downloading black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading open_clip_torch-2.23.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading timm-0.9.11-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading webdataset-0.2.79-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.41.3-py3-none-any.whl (65 kB)\n",
      "Downloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl (211.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pywavelets-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.36.0-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading urwid-2.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.4/274.4 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Using cached cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading aiohttp-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Downloading yarl-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: clip, pudb, urwid_readline\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369539 sha256=11d2bf53097fd826e0134b36186dea90e32af298b948b7f0cae8a8ca9f62a3fd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ka1th_11/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "  Building wheel for pudb (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pudb: filename=pudb-2023.1-py3-none-any.whl size=86158 sha256=cb3f700d41f3706b7c4c710e617b8d4b2c179c7d4e41fb0bc3d8921e0a9ebc4e\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/03/e9/97cb1ce91fb1601f85470b71f11fcd3c6617e81735ccd4460c\n",
      "  Building wheel for urwid_readline (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for urwid_readline: filename=urwid_readline-0.13-py3-none-any.whl size=7549 sha256=3fd819716fbb591cbcc64ab08051279516d542f874f25cd0f47ae46391070785\n",
      "  Stored in directory: /root/.cache/pip/wheels/58/1d/d8/20c6d76afd5bd205f5f95f19640df9a4e88fc6f1a4c25bb693\n",
      "Successfully built clip pudb urwid_readline\n",
      "Installing collected packages: wcwidth, tokenizers, ninja, cmake, braceexpand, appdirs, zipp, wheel, watchdog, validators, urwid, tzlocal, toml, termcolor, tenacity, setproctitle, sentry-sdk, protobuf, pillow, numpy, natsort, multidict, lightning-utilities, ftfy, frozenlist, docker-pycreds, chardet, cachetools, blinker, black, async-timeout, yarl, webdataset, urwid_readline, tensorboardx, scipy, PyWavelets, pydeck, pyarrow, opencv-python, importlib-metadata, fire, aiosignal, wandb, transformers, pudb, aiohttp, streamlit, torchmetrics, timm, xformers, torchdata, pytorch-lightning, open-clip-torch, kornia, invisible-watermark, fairscale, clip\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.6\n",
      "    Uninstalling wcwidth-0.2.6:\n",
      "      Successfully uninstalled wcwidth-0.2.6\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.37.1\n",
      "    Uninstalling wheel-0.37.1:\n",
      "      Successfully uninstalled wheel-0.37.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: black\n",
      "    Found existing installation: black 23.9.1\n",
      "    Uninstalling black-23.9.1:\n",
      "      Successfully uninstalled black-23.9.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.0\n",
      "    Uninstalling transformers-4.34.0:\n",
      "      Successfully uninstalled transformers-4.34.0\n",
      "Successfully installed PyWavelets-1.5.0 aiohttp-3.9.0 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.3 black-23.7.0 blinker-1.7.0 braceexpand-0.1.7 cachetools-5.3.2 chardet-5.1.0 clip-1.0 cmake-3.27.7 docker-pycreds-0.4.0 fairscale-0.4.13 fire-0.5.0 frozenlist-1.4.0 ftfy-6.1.3 importlib-metadata-6.8.0 invisible-watermark-0.2.0 kornia-0.6.9 lightning-utilities-0.10.0 multidict-6.0.4 natsort-8.4.0 ninja-1.11.1.1 numpy-1.26.2 open-clip-torch-2.23.0 opencv-python-4.6.0.66 pillow-10.1.0 protobuf-3.20.3 pudb-2023.1 pyarrow-14.0.1 pydeck-0.8.1b0 pytorch-lightning-2.0.1 scipy-1.11.4 sentry-sdk-1.36.0 setproctitle-1.3.3 streamlit-1.28.2 tenacity-8.2.3 tensorboardx-2.6 termcolor-2.3.0 timm-0.9.11 tokenizers-0.12.1 toml-0.10.2 torchdata-0.6.1 torchmetrics-1.2.0 transformers-4.19.1 tzlocal-5.2 urwid-2.2.3 urwid_readline-0.13 validators-0.22.0 wandb-0.16.0 watchdog-3.0.0 wcwidth-0.2.12 webdataset-0.2.79 wheel-0.41.3 xformers-0.0.22 yarl-1.9.3 zipp-3.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///app/generative-models\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sgm\n",
      "  Building editable for sgm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgm: filename=sgm-0.1.0-py3-none-any.whl size=26731 sha256=18592cbe32afd7d64233eda689174b86e8fe6826c0837680262f706321122a9b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-knlh1v9c/wheels/cf/f0/c4/e4ea11582b513cca8c76ad7b795b4dfc99010e39537ad66eab\n",
      "Successfully built sgm\n",
      "Installing collected packages: sgm\n",
      "  Attempting uninstall: sgm\n",
      "    Found existing installation: sgm 0.1.0\n",
      "    Uninstalling sgm-0.1.0:\n",
      "      Successfully uninstalled sgm-0.1.0\n",
      "Successfully installed sgm-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining sdata from git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n",
      "  Updating ./src/sdata clone (to revision main)\n",
      "  Running command git fetch -q --tags\n",
      "  Running command git reset --hard -q 8bce77d147033b3a5285b6d45ee85f33866964fc\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: sdata\n",
      "  Attempting uninstall: sdata\n",
      "    Found existing installation: sdata 0.0.1\n",
      "    Uninstalling sdata-0.0.1:\n",
      "      Successfully uninstalled sdata-0.0.1\n",
      "  Running setup.py develop for sdata\n",
      "Successfully installed sdata-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (5.1.2)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.104.1)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.17.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.2)\n",
      "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.1.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.2)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.9.2)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2022.12.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title Setup\n",
    "!nvidia-smi\n",
    "!git clone https://github.com/Stability-AI/generative-models.git\n",
    "# install required packages from pypi\n",
    "!pip3 install -r generative-models/requirements/pt2.txt\n",
    "# manually install only necesarry packages for colab\n",
    "#!wget https://gist.githubusercontent.com/mkshing/4ad40699756d996ba6b3f7934e6ca532/raw/3f0094272c7a2bd3eb5f1a0db91bed582c9e8f01/requirements.txt\n",
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b0687f-3027-4805-9bf6-24628114bf61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///app/generative-models\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sgm\n",
      "  Building editable for sgm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgm: filename=sgm-0.1.0-py3-none-any.whl size=26731 sha256=18592cbe32afd7d64233eda689174b86e8fe6826c0837680262f706321122a9b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qz8xgv3j/wheels/cf/f0/c4/e4ea11582b513cca8c76ad7b795b4dfc99010e39537ad66eab\n",
      "Successfully built sgm\n",
      "Installing collected packages: sgm\n",
      "  Attempting uninstall: sgm\n",
      "    Found existing installation: sgm 0.1.0\n",
      "    Uninstalling sgm-0.1.0:\n",
      "      Successfully uninstalled sgm-0.1.0\n",
      "Successfully installed sgm-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining sdata from git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n",
      "  Updating ./src/sdata clone (to revision main)\n",
      "  Running command git fetch -q --tags\n",
      "  Running command git reset --hard -q 8bce77d147033b3a5285b6d45ee85f33866964fc\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: sdata\n",
      "  Attempting uninstall: sdata\n",
      "    Found existing installation: sdata 0.0.1\n",
      "    Uninstalling sdata-0.0.1:\n",
      "      Successfully uninstalled sdata-0.0.1\n",
      "  Running setup.py develop for sdata\n",
      "Successfully installed sdata-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (5.1.2)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.104.1)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.17.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.2)\n",
      "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.1.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.2)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.9.2)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2022.12.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -e generative-models\n",
    "!pip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n",
    "!pip3 install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4cb284-f272-4154-b879-c0829c432512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.2\n",
      "Uninstalling numpy-1.26.2:\n",
      "  Successfully uninstalled numpy-1.26.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/64/41/284783f1014685201e447ea976e85fed0e351f5debbaf3ee6d7645521f1d/numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip uninstall -y numpy\n",
    "#!pip install -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891e870a-7f7a-416e-8483-e40c0258b362",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d887a553-877e-4750-946c-a1dd13ff8aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p /app/scripts/util/detection\n",
    "!ln -s /app/generative-models/scripts/util/detection/p_head_v1.npz /app/scripts/util/detection/p_head_v1.npz\n",
    "!ln -s /app/generative-models/scripts/util/detection/w_head_v1.npz /app/scripts/util/detection/w_head_v1.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89f0484-b40a-4fbe-ab07-e3db79e6dfff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = \"svd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73d94bc-c9d4-4158-9aa1-fa0e6d62692a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #3: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n",
      "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Restored from checkpoints/svd.safetensors with 0 missing and 0 unexpected keys\n"
     ]
    }
   ],
   "source": [
    "#@title Load Model\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"generative-models\")\n",
    "from sgm.util import default, instantiate_from_config\n",
    "from scripts.util.detection.nsfw_and_watermark_dectection import DeepFloydDataFiltering\n",
    "\n",
    "def load_model(\n",
    "    config: str,\n",
    "    device: str,\n",
    "    num_frames: int,\n",
    "    num_steps: int,\n",
    "):\n",
    "    config = OmegaConf.load(config)\n",
    "    config.model.params.conditioner_config.params.emb_models[\n",
    "        0\n",
    "    ].params.open_clip_embedding_config.params.init_device = device\n",
    "    config.model.params.sampler_config.params.num_steps = num_steps\n",
    "    config.model.params.sampler_config.params.guider_config.params.num_frames = (\n",
    "        num_frames\n",
    "    )\n",
    "    with torch.device(device):\n",
    "        model = instantiate_from_config(config.model).to(device).eval().requires_grad_(False)\n",
    "\n",
    "    filter = DeepFloydDataFiltering(verbose=False, device=device)\n",
    "    return model, filter\n",
    "\n",
    "\n",
    "if version == \"svd\":\n",
    "    num_frames = 14\n",
    "    num_steps = 25\n",
    "    # output_folder = default(output_folder, \"outputs/simple_video_sample/svd/\")\n",
    "    model_config = \"generative-models/scripts/sampling/configs/svd.yaml\"\n",
    "elif version == \"svd_xt\":\n",
    "    num_frames = 25\n",
    "    num_steps = 30\n",
    "    # output_folder = default(output_folder, \"outputs/simple_video_sample/svd_xt/\")\n",
    "    model_config = \"generative-models/scripts/sampling/configs/svd_xt.yaml\"\n",
    "else:\n",
    "    raise ValueError(f\"Version {version} does not exist.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, filter = load_model(\n",
    "    model_config,\n",
    "    device,\n",
    "    num_frames,\n",
    "    num_steps,\n",
    ")\n",
    "# move models expect unet to cpu\n",
    "model.conditioner.cpu()\n",
    "model.first_stage_model.cpu()\n",
    "# change the dtype of unet\n",
    "model.model.to(dtype=torch.float16)\n",
    "torch.cuda.empty_cache()\n",
    "model = model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132db413-119b-4bee-901b-336256a3c99f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Sampling function\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from fire import Fire\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "from sgm.inference.helpers import embed_watermark\n",
    "from sgm.util import default, instantiate_from_config\n",
    "\n",
    "\n",
    "def get_unique_embedder_keys_from_conditioner(conditioner):\n",
    "    return list(set([x.input_key for x in conditioner.embedders]))\n",
    "\n",
    "\n",
    "def get_batch(keys, value_dict, N, T, device, dtype=None):\n",
    "    batch = {}\n",
    "    batch_uc = {}\n",
    "\n",
    "    for key in keys:\n",
    "        if key == \"fps_id\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"fps_id\"]])\n",
    "                .to(device, dtype=dtype)\n",
    "                .repeat(int(math.prod(N)))\n",
    "            )\n",
    "        elif key == \"motion_bucket_id\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"motion_bucket_id\"]])\n",
    "                .to(device, dtype=dtype)\n",
    "                .repeat(int(math.prod(N)))\n",
    "            )\n",
    "        elif key == \"cond_aug\":\n",
    "            batch[key] = repeat(\n",
    "                torch.tensor([value_dict[\"cond_aug\"]]).to(device, dtype=dtype),\n",
    "                \"1 -> b\",\n",
    "                b=math.prod(N),\n",
    "            )\n",
    "        elif key == \"cond_frames\":\n",
    "            batch[key] = repeat(value_dict[\"cond_frames\"], \"1 ... -> b ...\", b=N[0])\n",
    "        elif key == \"cond_frames_without_noise\":\n",
    "            batch[key] = repeat(\n",
    "                value_dict[\"cond_frames_without_noise\"], \"1 ... -> b ...\", b=N[0]\n",
    "            )\n",
    "        else:\n",
    "            batch[key] = value_dict[key]\n",
    "\n",
    "    if T is not None:\n",
    "        batch[\"num_video_frames\"] = T\n",
    "\n",
    "    for key in batch.keys():\n",
    "        if key not in batch_uc and isinstance(batch[key], torch.Tensor):\n",
    "            batch_uc[key] = torch.clone(batch[key])\n",
    "    return batch, batch_uc\n",
    "\n",
    "\n",
    "\n",
    "def sample(\n",
    "    input_path: str = \"assets/test_image.png\",  # Can either be image file or folder with image files\n",
    "    resize_image: bool = False,\n",
    "    num_frames: Optional[int] = None,\n",
    "    num_steps: Optional[int] = None,\n",
    "    fps_id: int = 6,\n",
    "    motion_bucket_id: int = 127,\n",
    "    cond_aug: float = 0.02,\n",
    "    seed: int = 23,\n",
    "    decoding_t: int = 14,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
    "    device: str = \"cuda\",\n",
    "    output_folder: Optional[str] = \"/content/outputs\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple script to generate a single sample conditioned on an image `input_path` or multiple images, one for each\n",
    "    image file in folder `input_path`. If you run out of VRAM, try decreasing `decoding_t`.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    path = Path(input_path)\n",
    "    all_img_paths = []\n",
    "    if path.is_file():\n",
    "        if any([input_path.endswith(x) for x in [\"jpg\", \"jpeg\", \"png\"]]):\n",
    "            all_img_paths = [input_path]\n",
    "        else:\n",
    "            raise ValueError(\"Path is not valid image file.\")\n",
    "    elif path.is_dir():\n",
    "        all_img_paths = sorted(\n",
    "            [\n",
    "                f\n",
    "                for f in path.iterdir()\n",
    "                if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "            ]\n",
    "        )\n",
    "        if len(all_img_paths) == 0:\n",
    "            raise ValueError(\"Folder does not contain any images.\")\n",
    "    else:\n",
    "        raise ValueError\n",
    "    all_out_paths = []\n",
    "    for input_img_path in all_img_paths:\n",
    "        with Image.open(input_img_path) as image:\n",
    "            if image.mode == \"RGBA\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            if resize_image and image.size != (1024, 576):\n",
    "                print(f\"Resizing {image.size} to (1024, 576)\")\n",
    "                image = TF.resize(TF.resize(image, 1024), (576, 1024))\n",
    "            w, h = image.size\n",
    "\n",
    "            if h % 64 != 0 or w % 64 != 0:\n",
    "                width, height = map(lambda x: x - x % 64, (w, h))\n",
    "                image = image.resize((width, height))\n",
    "                print(\n",
    "                    f\"WARNING: Your image is of size {h}x{w} which is not divisible by 64. We are resizing to {height}x{width}!\"\n",
    "                )\n",
    "\n",
    "            image = ToTensor()(image)\n",
    "            image = image * 2.0 - 1.0\n",
    "\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        H, W = image.shape[2:]\n",
    "        assert image.shape[1] == 3\n",
    "        F = 8\n",
    "        C = 4\n",
    "        shape = (num_frames, C, H // F, W // F)\n",
    "        if (H, W) != (576, 1024):\n",
    "            print(\n",
    "                \"WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\"\n",
    "            )\n",
    "        if motion_bucket_id > 255:\n",
    "            print(\n",
    "                \"WARNING: High motion bucket! This may lead to suboptimal performance.\"\n",
    "            )\n",
    "\n",
    "        if fps_id < 5:\n",
    "            print(\"WARNING: Small fps value! This may lead to suboptimal performance.\")\n",
    "\n",
    "        if fps_id > 30:\n",
    "            print(\"WARNING: Large fps value! This may lead to suboptimal performance.\")\n",
    "\n",
    "        value_dict = {}\n",
    "        value_dict[\"motion_bucket_id\"] = motion_bucket_id\n",
    "        value_dict[\"fps_id\"] = fps_id\n",
    "        value_dict[\"cond_aug\"] = cond_aug\n",
    "        value_dict[\"cond_frames_without_noise\"] = image\n",
    "        value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n",
    "        value_dict[\"cond_aug\"] = cond_aug\n",
    "        # low vram mode\n",
    "        model.conditioner.cpu()\n",
    "        model.first_stage_model.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.sampler.verbose = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device):\n",
    "                model.conditioner.to(device)\n",
    "                batch, batch_uc = get_batch(\n",
    "                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n",
    "                    value_dict,\n",
    "                    [1, num_frames],\n",
    "                    T=num_frames,\n",
    "                    device=device,\n",
    "                )\n",
    "                c, uc = model.conditioner.get_unconditional_conditioning(\n",
    "                    batch,\n",
    "                    batch_uc=batch_uc,\n",
    "                    force_uc_zero_embeddings=[\n",
    "                        \"cond_frames\",\n",
    "                        \"cond_frames_without_noise\",\n",
    "                    ],\n",
    "                )\n",
    "                model.conditioner.cpu()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # from here, dtype is fp16\n",
    "                for k in [\"crossattn\", \"concat\"]:\n",
    "                    uc[k] = repeat(uc[k], \"b ... -> b t ...\", t=num_frames)\n",
    "                    uc[k] = rearrange(uc[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
    "                    c[k] = repeat(c[k], \"b ... -> b t ...\", t=num_frames)\n",
    "                    c[k] = rearrange(c[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
    "                for k in uc.keys():\n",
    "                    uc[k] = uc[k].to(dtype=torch.float16)\n",
    "                    c[k] = c[k].to(dtype=torch.float16)\n",
    "\n",
    "                randn = torch.randn(shape, device=device, dtype=torch.float16)\n",
    "\n",
    "                additional_model_inputs = {}\n",
    "                additional_model_inputs[\"image_only_indicator\"] = torch.zeros(\n",
    "                    2, num_frames\n",
    "                ).to(device, )\n",
    "                additional_model_inputs[\"num_video_frames\"] = batch[\"num_video_frames\"]\n",
    "\n",
    "                for k in additional_model_inputs:\n",
    "                    if isinstance(additional_model_inputs[k], torch.Tensor):\n",
    "                        additional_model_inputs[k] = additional_model_inputs[k].to(dtype=torch.float16)\n",
    "\n",
    "                def denoiser(input, sigma, c):\n",
    "                    return model.denoiser(\n",
    "                        model.model, input, sigma, c, **additional_model_inputs\n",
    "                    )\n",
    "\n",
    "                samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
    "                samples_z.to(dtype=model.first_stage_model.dtype)\n",
    "                ##\n",
    "\n",
    "                model.en_and_decode_n_samples_a_time = decoding_t\n",
    "                model.first_stage_model.to(device)\n",
    "                samples_x = model.decode_first_stage(samples_z)\n",
    "                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                model.first_stage_model.cpu()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                base_count = len(glob(os.path.join(output_folder, \"*.mp4\")))\n",
    "                video_path = os.path.join(output_folder, f\"{base_count:06d}.mp4\")\n",
    "                writer = cv2.VideoWriter(\n",
    "                    video_path,\n",
    "                    cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
    "                    fps_id + 1,\n",
    "                    (samples.shape[-1], samples.shape[-2]),\n",
    "                )\n",
    "\n",
    "                samples = embed_watermark(samples)\n",
    "                samples = filter(samples)\n",
    "                vid = (\n",
    "                    (rearrange(samples, \"t c h w -> t h w c\") * 255)\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    .astype(np.uint8)\n",
    "                )\n",
    "                for frame in vid:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    writer.write(frame)\n",
    "                writer.release()\n",
    "                all_out_paths.append(video_path)\n",
    "    return all_out_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf72335-82d3-4675-9e7f-5cb7010aa87e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://f1f8e3a1f817507cc2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f1f8e3a1f817507cc2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing (2560, 1920) to (1024, 576)\n",
      "##############################  Sampling setting  ##############################\n",
      "Sampler: EulerEDMSampler\n",
      "Discretization: EDMDiscretization\n",
      "Guider: LinearPredictionGuider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling with EulerEDMSampler for 26 steps:   0% 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Sampling with EulerEDMSampler for 26 steps:  96% 25/26 [00:55<00:02,  2.24s/it]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Your image is of size 498x960 which is not divisible by 64. We are resizing to 448x960!\n",
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n",
      "##############################  Sampling setting  ##############################\n",
      "Sampler: EulerEDMSampler\n",
      "Discretization: EDMDiscretization\n",
      "Guider: LinearPredictionGuider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling with EulerEDMSampler for 26 steps:   0% 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Sampling with EulerEDMSampler for 26 steps:  96% 25/26 [00:39<00:01,  1.59s/it]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n",
      "##############################  Sampling setting  ##############################\n",
      "Sampler: EulerEDMSampler\n",
      "Discretization: EDMDiscretization\n",
      "Guider: LinearPredictionGuider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling with EulerEDMSampler for 26 steps:   0% 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Sampling with EulerEDMSampler for 26 steps:  96% 25/26 [02:01<00:04,  4.87s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_18/1201385597.py\", line 11, in infer\n",
      "    output_paths = sample(\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 214, in sample\n",
      "    samples_x = model.decode_first_stage(samples_z)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/models/diffusion.py\", line 130, in decode_first_stage\n",
      "    out = self.first_stage_model.decode(\n",
      "  File \"/app/generative-models/sgm/models/autoencoder.py\", line 211, in decode\n",
      "    x = self.decoder(z, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 733, in forward\n",
      "    h = self.up[i_level].block[i_block](h, temb, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/autoencoding/temporal_ae.py\", line 70, in forward\n",
      "    x = super().forward(x, temb)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 134, in forward\n",
      "    h = nonlinearity(h)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 49, in nonlinearity\n",
      "    return x * torch.sigmoid(x)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 23.67 GiB total capacity; 17.36 GiB already allocated; 255.62 MiB free; 18.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_18/1201385597.py\", line 11, in infer\n",
      "    output_paths = sample(\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 214, in sample\n",
      "    samples_x = model.decode_first_stage(samples_z)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/models/diffusion.py\", line 130, in decode_first_stage\n",
      "    out = self.first_stage_model.decode(\n",
      "  File \"/app/generative-models/sgm/models/autoencoder.py\", line 211, in decode\n",
      "    x = self.decoder(z, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 733, in forward\n",
      "    h = self.up[i_level].block[i_block](h, temb, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/autoencoding/temporal_ae.py\", line 70, in forward\n",
      "    x = super().forward(x, temb)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 134, in forward\n",
      "    h = nonlinearity(h)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 49, in nonlinearity\n",
      "    return x * torch.sigmoid(x)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 23.67 GiB total capacity; 17.36 GiB already allocated; 255.62 MiB free; 18.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 23.67 GiB total capacity; 17.36 GiB already allocated; 255.62 MiB free; 18.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n",
      "##############################  Sampling setting  ##############################\n",
      "Sampler: EulerEDMSampler\n",
      "Discretization: EDMDiscretization\n",
      "Guider: LinearPredictionGuider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling with EulerEDMSampler for 26 steps:   0% 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Sampling with EulerEDMSampler for 26 steps:   0% 0/26 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_18/1201385597.py\", line 11, in infer\n",
      "    output_paths = sample(\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 208, in sample\n",
      "    samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 120, in __call__\n",
      "    x = self.sampler_step(\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 99, in sampler_step\n",
      "    denoised = self.denoise(x, denoiser, sigma_hat, cond, uc)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 55, in denoise\n",
      "    denoised = denoiser(*self.guider.prepare_inputs(x, sigma, cond, uc))\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 204, in denoiser\n",
      "    return model.denoiser(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/denoiser.py\", line 37, in forward\n",
      "    network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/wrappers.py\", line 28, in forward\n",
      "    return self.diffusion_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/video_model.py\", line 465, in forward\n",
      "    h = module(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/openaimodel.py\", line 93, in forward\n",
      "    x = layer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/video_attention.py\", line 281, in forward\n",
      "    x = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/attention.py\", line 546, in forward\n",
      "    return checkpoint(self._forward, x, context)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/app/generative-models/sgm/modules/attention.py\", line 571, in _forward\n",
      "    x = self.ff(self.norm3(x)) + x\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/attention.py\", line 113, in forward\n",
      "    return self.net(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/attention.py\", line 94, in forward\n",
      "    return x * F.gelu(gate)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 23.67 GiB total capacity; 16.96 GiB already allocated; 535.75 MiB free; 18.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_18/1201385597.py\", line 11, in infer\n",
      "    output_paths = sample(\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 208, in sample\n",
      "    samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 120, in __call__\n",
      "    x = self.sampler_step(\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 99, in sampler_step\n",
      "    denoised = self.denoise(x, denoiser, sigma_hat, cond, uc)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 55, in denoise\n",
      "    denoised = denoiser(*self.guider.prepare_inputs(x, sigma, cond, uc))\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 204, in denoiser\n",
      "    return model.denoiser(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/denoiser.py\", line 37, in forward\n",
      "    network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/wrappers.py\", line 28, in forward\n",
      "    return self.diffusion_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/video_model.py\", line 465, in forward\n",
      "    h = module(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/openaimodel.py\", line 93, in forward\n",
      "    x = layer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/video_attention.py\", line 281, in forward\n",
      "    x = block(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/attention.py\", line 546, in forward\n",
      "    return checkpoint(self._forward, x, context)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/app/generative-models/sgm/modules/attention.py\", line 571, in _forward\n",
      "    x = self.ff(self.norm3(x)) + x\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/attention.py\", line 113, in forward\n",
      "    return self.net(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/attention.py\", line 94, in forward\n",
      "    return x * F.gelu(gate)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 23.67 GiB total capacity; 16.96 GiB already allocated; 535.75 MiB free; 18.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 23.67 GiB total capacity; 16.96 GiB already allocated; 535.75 MiB free; 18.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_18/1201385597.py\", line 11, in infer\n",
      "    output_paths = sample(\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 170, in sample\n",
      "    c, uc = model.conditioner.get_unconditional_conditioning(\n",
      "  File \"/app/generative-models/sgm/modules/encoders/modules.py\", line 179, in get_unconditional_conditioning\n",
      "    c = self(batch_c, force_cond_zero_embeddings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/encoders/modules.py\", line 132, in forward\n",
      "    emb_out = embedder(batch[embedder.input_key])\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/encoders/modules.py\", line 1012, in forward\n",
      "    out = self.encoder.encode(vid[n * n_samples : (n + 1) * n_samples])\n",
      "  File \"/app/generative-models/sgm/models/autoencoder.py\", line 472, in encode\n",
      "    z = self.encoder(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 584, in forward\n",
      "    h = self.down[i_level].block[i_block](hs[-1], temb)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 134, in forward\n",
      "    h = nonlinearity(h)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 49, in nonlinearity\n",
      "    return x * torch.sigmoid(x)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 560.00 MiB (GPU 0; 23.67 GiB total capacity; 21.56 GiB already allocated; 193.75 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_18/1201385597.py\", line 11, in infer\n",
      "    output_paths = sample(\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 170, in sample\n",
      "    c, uc = model.conditioner.get_unconditional_conditioning(\n",
      "  File \"/app/generative-models/sgm/modules/encoders/modules.py\", line 179, in get_unconditional_conditioning\n",
      "    c = self(batch_c, force_cond_zero_embeddings)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/encoders/modules.py\", line 132, in forward\n",
      "    emb_out = embedder(batch[embedder.input_key])\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/encoders/modules.py\", line 1012, in forward\n",
      "    out = self.encoder.encode(vid[n * n_samples : (n + 1) * n_samples])\n",
      "  File \"/app/generative-models/sgm/models/autoencoder.py\", line 472, in encode\n",
      "    z = self.encoder(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 584, in forward\n",
      "    h = self.down[i_level].block[i_block](hs[-1], temb)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 134, in forward\n",
      "    h = nonlinearity(h)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 49, in nonlinearity\n",
      "    return x * torch.sigmoid(x)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 560.00 MiB (GPU 0; 23.67 GiB total capacity; 21.56 GiB already allocated; 193.75 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: CUDA out of memory. Tried to allocate 560.00 MiB (GPU 0; 23.67 GiB total capacity; 21.56 GiB already allocated; 193.75 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n",
      "##############################  Sampling setting  ##############################\n",
      "Sampler: EulerEDMSampler\n",
      "Discretization: EDMDiscretization\n",
      "Guider: LinearPredictionGuider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling with EulerEDMSampler for 26 steps:   0% 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Sampling with EulerEDMSampler for 26 steps:  96% 25/26 [00:58<00:02,  2.36s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_18/1201385597.py\", line 11, in infer\n",
      "    output_paths = sample(\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 214, in sample\n",
      "    samples_x = model.decode_first_stage(samples_z)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/models/diffusion.py\", line 130, in decode_first_stage\n",
      "    out = self.first_stage_model.decode(\n",
      "  File \"/app/generative-models/sgm/models/autoencoder.py\", line 211, in decode\n",
      "    x = self.decoder(z, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 733, in forward\n",
      "    h = self.up[i_level].block[i_block](h, temb, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/autoencoding/temporal_ae.py\", line 70, in forward\n",
      "    x = super().forward(x, temb)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 134, in forward\n",
      "    h = nonlinearity(h)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 49, in nonlinearity\n",
      "    return x * torch.sigmoid(x)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.34 GiB (GPU 0; 23.67 GiB total capacity; 20.06 GiB already allocated; 965.62 MiB free; 20.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_18/1201385597.py\", line 11, in infer\n",
      "    output_paths = sample(\n",
      "  File \"/tmp/ipykernel_18/4267746487.py\", line 214, in sample\n",
      "    samples_x = model.decode_first_stage(samples_z)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/models/diffusion.py\", line 130, in decode_first_stage\n",
      "    out = self.first_stage_model.decode(\n",
      "  File \"/app/generative-models/sgm/models/autoencoder.py\", line 211, in decode\n",
      "    x = self.decoder(z, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 733, in forward\n",
      "    h = self.up[i_level].block[i_block](h, temb, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/generative-models/sgm/modules/autoencoding/temporal_ae.py\", line 70, in forward\n",
      "    x = super().forward(x, temb)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 134, in forward\n",
      "    h = nonlinearity(h)\n",
      "  File \"/app/generative-models/sgm/modules/diffusionmodules/model.py\", line 49, in nonlinearity\n",
      "    return x * torch.sigmoid(x)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.34 GiB (GPU 0; 23.67 GiB total capacity; 20.06 GiB already allocated; 965.62 MiB free; 20.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: CUDA out of memory. Tried to allocate 2.34 GiB (GPU 0; 23.67 GiB total capacity; 20.06 GiB already allocated; 965.62 MiB free; 20.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# @title Do the Run!\n",
    "# @markdown Generation takes about 10 mins for `svd_xt` on T4 (Colab free plan). Please be patient...\n",
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "\n",
    "def infer(input_path: str, resize_image: bool, n_frames: int, n_steps: int, seed: str, decoding_t: int) -> str:\n",
    "  if seed == \"random\":\n",
    "    seed = random.randint(0, 2**32)\n",
    "  seed = int(seed)\n",
    "  output_paths = sample(\n",
    "    input_path=input_path,\n",
    "    resize_image=resize_image,\n",
    "    num_frames=n_frames,\n",
    "    num_steps=n_steps,\n",
    "    fps_id=6,\n",
    "    motion_bucket_id=127,\n",
    "    cond_aug=0.02,\n",
    "    seed=23,\n",
    "    decoding_t=decoding_t,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
    "    device=device,\n",
    "  )\n",
    "  return output_paths[0]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "  with gr.Column():\n",
    "    image = gr.Image(label=\"input image\", type=\"filepath\")\n",
    "    resize_image = gr.Checkbox(label=\"resize to optimal size\", value=True)\n",
    "    btn = gr.Button(\"Run\")\n",
    "    with gr.Accordion(label=\"Advanced options\", open=False):\n",
    "      n_frames = gr.Number(precision=0, label=\"number of frames\", value=num_frames)\n",
    "      n_steps = gr.Number(precision=0, label=\"number of steps\", value=num_steps)\n",
    "      seed = gr.Text(value=\"random\", label=\"seed (integer or 'random')\",)\n",
    "      decoding_t = gr.Number(precision=0, label=\"number of frames decoded at a time\", value=4)\n",
    "  with gr.Column():\n",
    "    video_out = gr.Video(label=\"generated video\")\n",
    "  examples = [\n",
    "      [\"https://user-images.githubusercontent.com/33302880/284758167-367a25d8-8d7b-42d3-8391-6d82813c7b0f.png\"]\n",
    "  ]\n",
    "  inputs = [image, resize_image, n_frames, n_steps, seed, decoding_t]\n",
    "  outputs = [video_out]\n",
    "  btn.click(infer, inputs=inputs, outputs=outputs)\n",
    "  gr.Examples(examples=examples, inputs=inputs, outputs=outputs, fn=infer)\n",
    "  demo.queue().launch(debug=True, share=True, show_error=True, server_name=\"0.0.0.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
