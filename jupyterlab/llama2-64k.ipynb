{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8930358f-c95c-4959-9ce0-e9fb102f3df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!mkdir -p /content\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b32377d-a22d-4c37-902e-8e0e04fc8ea5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  python-is-python3\n",
      "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
      "Need to get 2788 B of archives.\n",
      "After this operation, 13.3 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 python-is-python3 all 3.9.2-2 [2788 B]\n",
      "Fetched 2788 B in 1s (4663 B/s)             \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package python-is-python3.\n",
      "(Reading database ... 44398 files and directories currently installed.)\n",
      "Preparing to unpack .../python-is-python3_3.9.2-2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking python-is-python3 (3.9.2-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up python-is-python3 (3.9.2-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install python-is-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e555b34c-8488-4d6b-a507-8d1eda893946",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.32.1\n",
      "Uninstalling transformers-4.32.1:\n",
      "  Would remove:\n",
      "    /usr/local/bin/transformers-cli\n",
      "    /usr/local/lib/python3.10/dist-packages/transformers-4.32.1.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/transformers/*\n",
      "Proceed (Y/n)?   Successfully uninstalled transformers-4.32.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: optimum 1.12.0\n",
      "Uninstalling optimum-1.12.0:\n",
      "  Would remove:\n",
      "    /usr/local/bin/optimum-cli\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum-1.12.0.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/commands/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/configuration_utils.py\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/conftest.py\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/exporters/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/fx/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/gptq/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/modeling_base.py\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/onnx/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/onnxruntime/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/pipelines/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/quantization_base.py\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/runs_base.py\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/utils/*\n",
      "    /usr/local/lib/python3.10/dist-packages/optimum/version.py\n",
      "Proceed (Y/n)?   Successfully uninstalled optimum-1.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: auto-gptq 0.4.2+cu118\n",
      "Uninstalling auto-gptq-0.4.2+cu118:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.10/dist-packages/auto_gptq-0.4.2+cu118.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/auto_gptq/*\n",
      "    /usr/local/lib/python3.10/dist-packages/autogptq_cuda_256.cpython-310-x86_64-linux-gnu.so\n",
      "    /usr/local/lib/python3.10/dist-packages/autogptq_cuda_64.cpython-310-x86_64-linux-gnu.so\n",
      "    /usr/local/lib/python3.10/dist-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so\n",
      "Proceed (Y/n)?   Successfully uninstalled auto-gptq-0.4.2+cu118\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-s2k7s_0s\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-s2k7s_0s\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 0afa5071bd84e44301750fdc594e33db102cf374\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0.dev0) (2022.12.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.33.0.dev0-py3-none-any.whl size=7634320 sha256=8f338ed02e87defa4be239c895e25076a42f84d7b12c98f871fbb033b1023566\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4gkvcy17/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.33.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/huggingface/optimum.git\n",
      "  Cloning https://github.com/huggingface/optimum.git to /tmp/pip-req-build-3b664wno\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum.git /tmp/pip-req-build-3b664wno\n",
      "  Resolved https://github.com/huggingface/optimum.git to commit b771e04f8d36a9797a6826f8865b810f9e6badff\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.1.dev0) (15.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.1.dev0) (1.11.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.1.dev0) (4.33.0.dev0)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.1.dev0) (2.0.1+cu118)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.1.dev0) (23.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.1.dev0) (1.24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.1.dev0) (0.16.4)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.1.dev0) (2.14.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (4.7.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum==1.12.1.dev0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum==1.12.1.dev0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum==1.12.1.dev0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum==1.12.1.dev0) (3.27.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->optimum==1.12.1.dev0) (15.0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.12.1.dev0) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.12.1.dev0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.12.1.dev0) (0.3.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.12.1.dev0) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.12.1.dev0) (3.20.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum==1.12.1.dev0) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.1.dev0) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.1.dev0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.1.dev0) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.1.dev0) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.1.dev0) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.1.dev0) (3.8.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum==1.12.1.dev0) (1.2.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.1.dev0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.1.dev0) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.1.dev0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.1.dev0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.1.dev0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.1.dev0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.12.1.dev0) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum==1.12.1.dev0) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum==1.12.1.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum==1.12.1.dev0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum==1.12.1.dev0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.12.1.dev0) (1.16.0)\n",
      "Building wheels for collected packages: optimum\n",
      "  Building wheel for optimum (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum: filename=optimum-1.12.1.dev0-py3-none-any.whl size=389843 sha256=c8f1e483f044231cc89c4db4317d5b8e15ea11d556ab6563a5fbc30eddad452f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6h8_145d/wheels/8f/fa/71/cd84bac993f8fa15de35a1c058b91b9394bbc638e56c4327fd\n",
      "Successfully built optimum\n",
      "Installing collected packages: optimum\n",
      "Successfully installed optimum-1.12.1.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
      "Collecting auto-gptq\n",
      "  Using cached https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.4.2%2Bcu118-cp310-cp310-linux_x86_64.whl (1.9 MB)\n",
      "Requirement already satisfied: accelerate>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.22.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.14.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.24.1)\n",
      "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.3.3)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.33.0.dev0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.27.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (15.0.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.8.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.8.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.2.1)\n",
      "Installing collected packages: auto-gptq\n",
      "Successfully installed auto-gptq-0.4.2+cu118\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!echo y | pip3 uninstall transformers\n",
    "!echo y | pip3 uninstall optimum\n",
    "!echo y | pip3 uninstall auto-gptq\n",
    "\n",
    "#!pip3 install transformers>=4.32.0 optimum>=1.12.0\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/optimum.git\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602bee3-9657-4288-ad3e-cb133d8339fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade accelerate\n",
    "# !echo y | pip3 uninstall bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2092a4-365e-4ce7-a644-a0a53e98ba78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    }
   ],
   "source": [
    "# 最初の一度だけ実行\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Yarn-Llama-2-13B-64K-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             # load_in_4bit=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             revision=\"main\",\n",
    "                                            )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ec0286-b26e-42b3-86cd-b189db295cef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "248584\n",
      "Result: 次の学術論文を読みます。\n",
      "\n",
      " Llama 2 : Open Foundation and Fine-Tuned Chat ModelsHugo Touvron∗Louis Martin†Kevin Stone†Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya BatraPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya ChenGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian FullerCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui HouHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem KorenevPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana LiskovichYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar MishraIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan SaladiAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh TangRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen ZhangAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert StojnicSergey Edunov Thomas Scialom∗GenAI, MetaAbstractIn this work, we develop and release Llama 2, a collection of pretrained and fine-tunedlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Ourmodels outperform open-source chat models on most benchmarks we tested, and based onourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-source models. We provide a detailed description of our approach to fine-tuning and safetyimprovements of Llama 2-Chat in order to enable the community to build on our work andcontribute to the responsible development of LLMs.∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com†Second authorContributions for all the authors can be found in Section A.1.Contents1 Introduction 32 Pretraining 52.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52.3 Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 Fine-tuning 83.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93.2 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . . 93.3 System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 Safety 204.1 Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295 Discussion 325.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325.2 Limitations and Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345.3 Responsible Release Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 Related Work 357 Conclusion 36A Appendix 46A.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46A.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47A.3 Additional Details for Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51A.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58A.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72A.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75A.7 Model Card . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 772Figure 1: Helpfulness human evaluation results for Llama2-Chatcomparedtootheropen-sourceandclosed-sourcemodels. Human raters compared model generations on ~4kpromptsconsistingofbothsingleandmulti-turnprompts.The95%confidenceintervalsforthisevaluationarebetween1%and2%. MoredetailsinSection3.4.2. Whilereviewingthese results, it is important to note that human evaluationscanbenoisyduetolimitationsofthepromptset,subjectivityof the review guidelines, subjectivity of individual raters,and the inherent difficulty of comparing generations.Figure 2: Win-rate % for helpfulness andsafety between commercial-licensed base-lines and Llama 2-Chat , according to GPT-4. Tocomplementthehumanevaluation,weused a more capable model, not subject toourownguidance. Greenareaind \n",
      "\n",
      " この論文の学術的貢献を以下の文章にまとめようと思います。\n",
      "\n",
      " Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
      " Hugotouvron∗LouisMartin†KevinStone†PeterAlbertAmjadAlmahairiYasmineBabaeiNikolayBashlykovSoumyaBhatrasPrajjwalBhargavaShrutiBhosaleDanBikelLukasBlecherCristianCantonFernandezMoyaChenguillemCucurullDavidEsiobuJudeFernandesJeremyFuWenyinfuBrianFullerCyndiaGaoVedanujGoswamiNamanGoyalAnthonyHar\n",
      "\n",
      " tshumansagharHosseiniruiHouseHakanInanMarcinKardasViktorkerkezMadianKhabsaIsabellaKloumannArtemKorenevPunitSinghKouraMarie-AnneLachauxThibautLavrilJenyaLeeDianaLiskovichYinghaLuYuningMaoxavierMartinetTodorMihaylo\n",
      "\n",
      " vPushkarMishraIgorMolybogYixinNieAndrewPoultonJeremyReizensteinRashiRungtaKalyanSaladiAlanScheltenRuansilvaEricMichaelSmithRanjanSubramanianXiaoqingEllenTanBinhTangRossTaylorAdinaWilliamsJianXiangKua\n",
      "\n",
      " nPuxinxZhengyanIliyanZarovYuchenzhangAngelaFanMelanieKambadurSharanNarangAurelienRodriguezRobertStojnicSergeyEdunovThomasScialom∗GenAIMetaAbstractThisworkdevelopsandreleasesLlama2\n",
      "\n",
      " ancollectionofpretrainedandfine-tunedlargelanguage models(LLMs)ranginginscalesfrom7billiontot70bil lionparameters Ourlarinemodelgeneratorsoutperformopen-sourcemo\n",
      "\n",
      " delsonthebenchmarkswe testedandbasedonourhumanevaluationsforhelpfulnessandsafetymaybeasuitablesubstitutesforclosesourcesystems Wewavedetaileddescrip\n",
      "\n",
      " tionoffineteuningandsafetyimprovementsoflLlama2-Chatintordiscouragethecommunitytobuildonourworkandcontributeto theresponsibledevelopmentofLLMs ∗E qu al c\n",
      "\n",
      " ontributionallcontributorssecondauthors: GenAI MetaGenAI MetaAbstractInthisworkwedevelopandreleaseLlama2acollectionofpretrainedandfinetunedlarge la\n",
      "\n",
      " ngmodelsscaledfrom7billiontop70bil lionparameters Ourfine-tunedLLMscalledLlama2-ChatareoptimizedfordialogueusecasesOurlarinemodelspretrainedwithsupervis\n",
      "\n",
      " ionandfine-tunedwithRLHFoutperformedothermodelsevenonthenlargestbenchmar kdata setsWealsofoundthatLLamashowcasehigherlevelsofunderstandingandreasoningt han o\n",
      "\n",
      " therpre-trainingapproaches Weprovidadetaileddescriptionofourapproachtosupervisedandreinforcementlearningforfine-tuningLLMsandadditionalsafetyimproveme\n",
      "\n",
      " ntstoenabletheres ponsibilitycomm unitytobuildresponsiblyonourworkWeinvestigateandreportonthethreekeychallengesinresponsiblechatbotdeve lopm ent:\n",
      "\n",
      " safepretrai ning safefin e-t uning and red teaming Llama 2-Chat is built upon LLa ma1whichwas releasedearlierthisyearWehaveupgradedLLama1withlargerpretrai ned\n",
      "\n",
      " m od els larger finetuning datasets and improved training techniques ThispaperdescribesthespecificchangesmadeforLLama2andhowtheyrelate to the overall cha\n",
      "\n",
      " llengesfacedbyLLMA developers The paper also describes how these changes impact safety and discussessuggestionsforthefurther improvement of LLa ma’s safety in\n",
      "\n",
      " the future We hope that sharing this information will help other researchers improve their models by building up on our work We describe the following topics\n",
      "\n",
      "  Pre-training details: scaling and data selection\n",
      "  Finetuning details: supervised fine-tuning and reinforcement learning with human feedback\n",
      "  Safety improvements: system message, safety\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./llama2論文.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(len(documents))\n",
    "document=\"\"\n",
    "for doc in documents:\n",
    "    document+=doc.page_content\n",
    "text=document.replace(\"\\n\",\"\")\n",
    "print(len(text))\n",
    "\n",
    "text = text[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a9a299-2c72-4500-a751-181c0a20ff48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "39189\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./nextjs-vs-remixjs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(len(documents))\n",
    "document=\"\"\n",
    "for doc in documents:\n",
    "    document+=doc.page_content\n",
    "text=document.replace(\"\\n\",\"\")\n",
    "print(len(text))\n",
    "\n",
    "text = text[:10000]\n",
    "\n",
    "del pipe\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "question=\"Which is better, Next.js or Remix.js?\"\n",
    "sequences = pipe(\n",
    "    f\"Read the following article and answer the Questions.\\n\\n{text}\\n\\n{question}\\n\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # max_length=5000,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21f1502d-6683-4861-b11b-adf5999459f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 2347\n",
      "Result: Read the following article and answer the Questions.\n",
      "\n",
      "Synopsis of Unemployed ReincarnationThe main character, 34 years old, unemployed, NEET and living a reclusive life, is run over by a truck one day and dies.He finds himself reincarnated as a baby in a different world reminiscent of medieval Europe.His name is Ludeus Greylat, and he is determined to live a good life the next time.From a young age, he read many books and practiced magic.His parents saw that he had talent and brought in Roxy Migurdia as a magical tutor.Later, after having two younger sisters and completing his magical training, Rudaeus began to worry about his own development as he grew older.He asked his father, Paulo, if he and his friend Sylphiette, a beautiful girl, could attend a magic college, but his father would not allow it and sent them to the fortress city of Roa.Rudeus is sent to the Citadel of Roa to work as a tutor for the young but violent Elise Boreas Greylat in order to earn money to enter the magic school.After a while, the two became good friends and were in the wilderness with Guilene, Eris's swordswoman escort.Suddenly, they are swallowed by a light that extends from the sky.Then, the territory of Fitoor, including the three of them, is caught up in a mysterious disaster.In a dream, Rudeus was advised by a man who claimed to be the God of Man to \"rely on the man nearby to help you when you wake up,\" and he decided to help Ruijerd of the Speld tribe who was nearby when he woke up in the Demon Continent, as advised.The group finally arrives at the port town of Wenport, where they board a ship to land on the Myrris continent and reunite with their father.There they learn that the territory of Fitua has disappeared, that a large number of people have been killed, and that those who were relocated have been enslaved. My father was involved in a campaign to free the enslaved people.Even though the Fitua territory became a prairie, Rudaeus did not change his destination.Rudius did not change his destination, because he had a strong desire to send Eris back safely.The group managed to return to the Fitua territory, engaging in a battle with a man named Olsted along the way, but there they were told by the escort swordsman Guilene that Elise's parents and grandfather had died.Eris, who was depressed but still wanted to be of help to Rudeus, decided to go on a journey with Guilene.\n",
      "\n",
      "The dead are listed below.\n",
      "\n",
      "* Paulo Greylat (deceased)\n",
      "* Sylphietta Greylat (deceased)\n",
      "* Elise Boreas Greylat (deceased)\n",
      "* Eris Boreas Greylat (deceased)\n",
      "* Guilene (deceased)\n",
      "* Olsted (deceased)\n",
      "\n",
      "### Characters\n",
      "\n",
      "Rudeus Greyrat: The protagonist of the story, a boy born into a noble family. He is an orphan whose mother died giving birth to him and whose father died shortly afterwards. His only relative is his uncle, who raised him until he was eight years old. At the age of ten, he was taken under the wing of a powerful mage named Roxy Migurdia, who taught him how to use magic. After learning magic, he went on adventures with her and other companions. In the end, he saved the world from destruction.\n",
      "\n",
      "Elze: A girl who lives in a village near the capital. She is the daughter of a wealthy merchant and has always been spoiled by her father. However, she is also very kind-hearted and loving towards others. When she meets Rudeus,\n",
      "Result: Read the following article and answer the Questions.\n",
      "\n",
      "Synopsis of Unemployed ReincarnationThe main character, 34 years old, unemployed, NEET and living a reclusive life, is run over by a truck one day and dies.He finds himself reincarnated as a baby in a different world reminiscent of medieval Europe.His name is Ludeus Greylat, and he is determined to live a good life the next time.From a young age, he read many books and practiced magic.His parents saw that he had talent and brought in Roxy Migurdia as a magical tutor.Later, after having two younger sisters and completing his magical training, Rudaeus began to worry about his own development as he grew older.He asked his father, Paulo, if he and his friend Sylphiette, a beautiful girl, could attend a magic college, but his father would not allow it and sent them to the fortress city of Roa.Rudeus is sent to the Citadel of Roa to work as a tutor for the young but violent Elise Boreas Greylat in order to earn money to enter the magic school.After a while, the two became good friends and were in the wilderness with Guilene, Eris's swordswoman escort.Suddenly, they are swallowed by a light that extends from the sky.Then, the territory of Fitoor, including the three of them, is caught up in a mysterious disaster.In a dream, Rudeus was advised by a man who claimed to be the God of Man to \"rely on the man nearby to help you when you wake up,\" and he decided to help Ruijerd of the Speld tribe who was nearby when he woke up in the Demon Continent, as advised.The group finally arrives at the port town of Wenport, where they board a ship to land on the Myrris continent and reunite with their father.There they learn that the territory of Fitua has disappeared, that a large number of people have been killed, and that those who were relocated have been enslaved. My father was involved in a campaign to free the enslaved people.Even though the Fitua territory became a prairie, Rudaeus did not change his destination.Rudius did not change his destination, because he had a strong desire to send Eris back safely.The group managed to return to the Fitua territory, engaging in a battle with a man named Olsted along the way, but there they were told by the escort swordsman Guilene that Elise's parents and grandfather had died.Eris, who was depressed but still wanted to be of help to Rudeus, decided to go on a journey with Guilene.\n",
      "\n",
      "The names of the persons are listed below.\n",
      "\n",
      "* Rudeus Greyrat (ルデウス・グレイラット) - The protagonist of the story. He is a boy born into a noble family. His mother died shortly after giving birth to him, so he was raised by his father, Paul, and his stepmother, Zenith. He is an excellent student and a talented mage. He is also very intelligent and can easily understand things. He is kind-hearted and always tries to do what is right. He is also very loyal to his friends and will do anything to protect them. He is currently studying at the Royal Academy of Magic.\n",
      "\n",
      "* Eris Boreas Greyrat (エリス・ボアーズ・グレイラット) - A girl who lives in the same village as Rudeus. She is the daughter of the local lord and is known for her beauty. She is also very smart and has a great sense of justice. She is a close friend of Rudeus and often helps him out with his studies. She is currently attending the Royal Academy of Magic.\n",
      "\n",
      "* Sylphyetia (シルフィエティア) - A girl who lives in the\n",
      "Result: Read the following article and answer the Questions.\n",
      "\n",
      "Synopsis of Unemployed ReincarnationThe main character, 34 years old, unemployed, NEET and living a reclusive life, is run over by a truck one day and dies.He finds himself reincarnated as a baby in a different world reminiscent of medieval Europe.His name is Ludeus Greylat, and he is determined to live a good life the next time.From a young age, he read many books and practiced magic.His parents saw that he had talent and brought in Roxy Migurdia as a magical tutor.Later, after having two younger sisters and completing his magical training, Rudaeus began to worry about his own development as he grew older.He asked his father, Paulo, if he and his friend Sylphiette, a beautiful girl, could attend a magic college, but his father would not allow it and sent them to the fortress city of Roa.Rudeus is sent to the Citadel of Roa to work as a tutor for the young but violent Elise Boreas Greylat in order to earn money to enter the magic school.After a while, the two became good friends and were in the wilderness with Guilene, Eris's swordswoman escort.Suddenly, they are swallowed by a light that extends from the sky.Then, the territory of Fitoor, including the three of them, is caught up in a mysterious disaster.In a dream, Rudeus was advised by a man who claimed to be the God of Man to \"rely on the man nearby to help you when you wake up,\" and he decided to help Ruijerd of the Speld tribe who was nearby when he woke up in the Demon Continent, as advised.The group finally arrives at the port town of Wenport, where they board a ship to land on the Myrris continent and reunite with their father.There they learn that the territory of Fitua has disappeared, that a large number of people have been killed, and that those who were relocated have been enslaved. My father was involved in a campaign to free the enslaved people.Even though the Fitua territory became a prairie, Rudaeus did not change his destination.Rudius did not change his destination, because he had a strong desire to send Eris back safely.The group managed to return to the Fitua territory, engaging in a battle with a man named Olsted along the way, but there they were told by the escort swordsman Guilene that Elise's parents and grandfather had died.Eris, who was depressed but still wanted to be of help to Rudeus, decided to go on a journey with Guilene.\n",
      "\n",
      "I taught Rudeus magic,\n",
      "\n",
      "and I also taught him how to use a sword.\n",
      "\n",
      "When we first met, he was just an ordinary boy.\n",
      "\n",
      "But now...\n",
      "\n",
      "Now he's become quite extraordinary.\n",
      "\n",
      "\n",
      "1. What kind of personality does Rudeus have?\n",
      "\n",
      "2. Why do you think Rudeus wants to study magic?\n",
      "\n",
      "3. How does Rudeus feel about being reborn into another world?\n",
      "\n",
      "4. Do you think Rudeus will ever find out what happened to his family?\n",
      "\n",
      "5. What do you think will happen to Rudeus in this new world?\n",
      "\n",
      "6. Who do you think Rudeus will meet next?\n",
      "\n",
      "7. What do you think Rudeus will do next?\n",
      "\n",
      "8. What do you think Rudeus will say to her?\n",
      "\n",
      "9. What do you think Rudeus will ask her?\n",
      "\n",
      "10. What do you think Rudeus will tell her?\n",
      "\n",
      "11. What do you think Rudeus will do next?\n",
      "\n",
      "12. What do you think Rudeus will say to her?\n",
      "\n",
      "13. What do you think Rudeus will ask her?\n"
     ]
    }
   ],
   "source": [
    "del pipe\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./mushoku.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "document=\"\"\n",
    "for doc in documents:\n",
    "    document+=doc.page_content\n",
    "text=document.replace(\"\\n\",\"\")\n",
    "print(f\"Document length: {len(text)}\")\n",
    "\n",
    "text = text[:10000]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "question=\"The dead are listed below.\"\n",
    "sequences = pipe(\n",
    "    f\"Read the following article and answer the Questions.\\n\\n{text}\\n\\n{question}\\n\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # max_length=5000,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n",
    "    \n",
    "question=\"The names of the persons are listed below.\"\n",
    "sequences = pipe(\n",
    "    f\"Read the following article and answer the Questions.\\n\\n{text}\\n\\n{question}\\n\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # max_length=5000,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n",
    "    \n",
    "    \n",
    "question=\"I taught Rudeus magic,\"\n",
    "sequences = pipe(\n",
    "    f\"Read the following article and answer the Questions.\\n\\n{text}\\n\\n{question}\\n\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # max_length=5000,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd4caee-add4-46fb-b4d2-24c576cd47a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 47361\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.51 GiB (GPU 0; 23.67 GiB total capacity; 17.66 GiB already allocated; 1.42 GiB free; 20.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     17\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.15\u001b[39m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser:What is the name of the manager of the live house \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mSTARRY\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRead the following article and answer the Questions.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquestion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAssistant:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_length=5000,\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py:205\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py:268\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1641\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1642\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1643\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1644\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1645\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2727\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2730\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2731\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2734\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2738\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/TheBloke/Yarn-Llama-2-13B-64K-GPTQ/ce7e3298f2972f0567fb6959222d8ae33d26b718/modeling_llama_together_yarn.py:1004\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, is_padded_inputs)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[0;32m-> 1004\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.51 GiB (GPU 0; 23.67 GiB total capacity; 17.66 GiB already allocated; 1.42 GiB free; 20.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "del pipe\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./bocchi.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "document=\"\"\n",
    "for doc in documents:\n",
    "    document+=doc.page_content\n",
    "text=document.replace(\"\\n\",\"\")\n",
    "print(f\"Document length: {len(text)}\")\n",
    "\n",
    "# text = text[:20000]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "question=\"User:What is the name of the manager of the live house \\\"STARRY\\\"?\"\n",
    "sequences = pipe(\n",
    "    f\"Read the following article and answer the Questions.\\n\\n{text}\\n\\n{question}\\nAssistant:\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # max_length=5000,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
