{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c8cb2-8284-4e66-bd2c-05ae87137645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82976cd0-4d57-4319-b2ea-40f0980cca76",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rinna/bilingual-gpt-neox-4b-8k\", use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"rinna/bilingual-gpt-neox-4b-8k\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04597fbd-9880-4f82-9d75-755bf96a47ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(text):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            token_ids.to(model.device),\n",
    "            max_new_tokens=1000,\n",
    "            min_new_tokens=1000,\n",
    "            do_sample=True,\n",
    "            temperature=1.0,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids.tolist()[0])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a82453-68a5-400a-aaa8-50c9d5024f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昔々あるところに、宇宙海賊ピーチボーイという悪者によって惑星が侵略され、その惑星を救うためにピーチボーイの基地で仲間として戦うことになったのでございます。\n",
      "そのピーチボーイが宇宙海賊をしていて宇宙の悪の帝王(悪の首領)であるボスが惑星と宇宙を自分の好きなようにしようとしているのでそれを退治するために、宇宙怪盗としてピーチボーイに雇われたヒーロー(ヒーローはピーチボーイを宇宙で戦って、ピーチボーイは宇宙でヒーローたちを戦います)が宇宙に散らばっている様々な星からピーチボーイのもとにやってきます。\n",
      "この物語はヒーローたちがピーチボーイに雇われて宇宙の悪と対決するストーリーです。\n",
      "ピーチボーイは宇宙の悪と戦いながらも宇宙怪盗としてはピーチボーイに雇われたヒーローたちを地球に派遣してピーチボーイの縄張りを荒らさせ、その縄張りで悪の悪者(悪のボス)と戦っているヒーローたちをピーチボーイに頼んで守らせ、ピーチボーイが縄張りで倒した悪の悪者(悪のボス)を地球に送ってヒーローたちの戦いを助けさせようとします。\n",
      "悪の悪者にピーチボーイの縄張りを荒らされていることを知らされたピーチボーイは地球に行って悪の悪者(悪のボス)を退治するのですが、悪の悪者が悪の悪者(悪のボス)を地球の周りに集結してピーチボーイと対決しているのですが、それは地球の周りのピーチボーイの縄張りに悪の悪者が集結してピーチボーイが善のヒーローたちを守れないようにして、ピーチボーイが縄張りを侵略してきた悪者(悪の悪者のボス)を退治しているのをヒーローがピーチボーイを応援して悪の悪者(悪のボス)を退治するストーリーです。\n",
      "悪の悪者に悪の悪者(悪のボス)を退治されて、地球はピーチボーイの縄張りに守られ善のヒーローたちを守れることがわかって、ピーチボーイは悪の悪者(悪のボス)を退治してヒーローたちを守りました。\n",
      "ピーチボーイは正義のヒーローが悪の悪者(悪のボス)を退治してヒーローたちが地球を守るために悪と戦っているのを応援し、悪の悪の勢力を退治することでヒーローたちを守っていることがわかります。\n",
      "宇宙海賊ピーチボーイ物語(全13巻)\n",
      "宇宙海賊ピーチボーイ2(全10巻)\n",
      "宇宙海賊ピーチボーイ(全14巻)\n",
      "1981年12月12日〜12月19日に、日本テレビ系列の「火曜サスペンス劇場」で放映。\n",
      "企画:渡辺由自(日本テレビ)\n",
      "プロデューサー:西本正成(日本テレビ)、加藤信\n",
      "制作:日本テレビ 出演 宇宙海賊ピーチボーイ - 渡辺謙 宇宙海賊ピーチボーイ2 - 中山仁 宇宙海賊ピーチボーイ - 夏夕介 宇宙海賊ピーチボーイ2 - 中村晃子 宇宙海賊ピーチボーイ - 風間杜夫 宇宙海賊ピーチボーイ2 - 斉藤晴彦 宇宙海賊ピーチボーイ - 夏亜矢子 宇宙海賊ピーチボーイ - 鈴木光枝 宇宙海賊ピーチボーイ2 - 松本嘉代 宇宙海賊ピーチボーイ - 鈴木隆 宇宙海賊ピーチボーイ2 - 森沢なつ 宇宙海賊ピーチボーイ - 伊藤雄之助 宇宙海賊ピーチボーイ2 - 鈴木清順 宇宙海賊ピーチボーイ - 藤岡重慶 宇宙海賊ピーチボーイ2 - 小林昭二 1997年3月10日〜3月16日にテレビ朝日系列の「土曜ワイド劇場」で放映。\n",
      "企画:松本陽一(テレビ朝日)\n",
      "プロデューサー:五十嵐衛(テレビ朝日)\n",
      "制作:テレビ朝日 原作:永井良治、渡辺謙造 脚本:田辺満(第1話)、山田耕大(第2話)、井上昭(第3話)、森島直輝(第4話)、山本有(第5話)、保利吉彦(第6話)、鎌田敏夫(第7話)、酒井彰(第8話) 音楽:服部克久 主題歌:「宇宙海賊」クリスタル・ゲイル(日本クラウン) 作曲:渡辺博也 演奏:渡辺博也とクリスタル・ゲイルとスーパー・ストリングス(ストリングス)(日本クラウン) 録音スタジオ:にっかつスタジオ 照明:小林恒雄 美術:松沢由之 装飾:高橋俊春 衣装:岩谷孝子 特殊効果:山本武造 スクリプター:加藤真知子 編集:平木美和 音響効果:東洋音響 助監督:佐藤健 監督助手:田部井克 進行:加藤由子 スチール:岩井田浩一 記録:堀善郎 現像・映像(第1話):東洋現像所(現 IMAGICA) 撮影協力:海上保安庁、宇宙科学研究所(JAXA) 技術提供:日産自動車(第1・2話)、日産自動車(第3・4話)、日産自動車(第5話)、スズキ(第6\n"
     ]
    }
   ],
   "source": [
    "text = \"昔々あるところに、宇宙海賊ピーチボーイという\"\n",
    "inference(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81d624-536b-4b05-a4fb-2830dc87a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct2版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da97334b-13ac-4f68-8cbd-055b7dd54342",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ctranslate2 in /usr/local/lib/python3.10/dist-packages (3.17.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2) (1.24.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2) (6.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (4.23.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ctranslate2\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a31455-b1e3-4ba9-af88-40a47c709c1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "16K\t./8k.ipynb\n",
      "12K\t./minigpt-4.ipynb\n",
      "3.6G\t./rinna-bilingual-gpt-neox-4b-8k-ct2\n"
     ]
    }
   ],
   "source": [
    "!rm -rf rinna-bilingual-gpt-neox-4b-8k-ct2\n",
    "!ct2-transformers-converter --model rinna/bilingual-gpt-neox-4b-8k --quantization int8_float16 --output_dir rinna-bilingual-gpt-neox-4b-8k-ct2\n",
    "!du -sh ./*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a90864-2fdb-4a6b-8f4a-ad8b24f2086c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE: cuda\n",
      "システム(rinnna-8k): 昔々あるところに。。。。。;;わずともず地果てていることていることこの世<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<↓↓!!ささ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆こんなどっちどっちどっちどっちどっちどっち\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'昔々あるところに。。。。。;;わずともず地果てていることていることこの世<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<↓↓!!ささ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆こんなどっちどっちどっちどっちどっちどっち'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctranslate2\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model = \"rinna/bilingual-gpt-neox-4b-8k\"\n",
    "ct2_model= model.replace(\"/\", \"-\") + \"-ct2\"\n",
    "\n",
    "# cudaが使える場合はcudaを使う\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"USE: {device}\")\n",
    "\n",
    "# generator = ctranslate2.Generator(\"rinna_gpt_neox_ppo_ct2_ft16\", device=device)\n",
    "generator = ctranslate2.Generator(ct2_model, device=device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model, use_fast=False, legacy=False)\n",
    "\n",
    "\n",
    "# 返信を作成する\n",
    "def inference(msg):\n",
    "    p = msg\n",
    "    tokens = tokenizer.convert_ids_to_tokens(\n",
    "        tokenizer.encode(\n",
    "            p,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = generator.generate_batch(\n",
    "        [tokens],\n",
    "        max_length=100,\n",
    "        min_length=100,\n",
    "        sampling_topk=0,\n",
    "        sampling_temperature=1.0,\n",
    "        include_prompt_in_result=True,\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(results[0].sequences_ids[0])\n",
    "    print(\"システム(rinnna-8k): \" + text + \"\\n\")\n",
    "    return text\n",
    "\n",
    "inference(\"昔々あるところに\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
