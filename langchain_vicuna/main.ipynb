{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac24e7d-9870-4751-aa0b-5132a9573ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「OpenAI API無しでLangChainを使う方法を解説」を参考にしている\n",
    "https://www.youtube.com/watch?v=U0QOgjYwspc&ab_channel=%E3%81%AB%E3%82%83%E3%82%93%E3%81%9F%E3%81%AEAI%E5%AE%9F%E8%B7%B5%E3%83%81%E3%83%A3%E3%83%B3%E3%83%8D%E3%83%AB%E3%80%90Python%C3%97%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%C3%97ChatGPT%E3%80%91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51388e2e-b452-4867-93fb-995a328d13dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /content\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8591d4-cf22-4aad-b92f-e341a49d85ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libaria2-0 libc-ares2 libssh2-1\n",
      "The following NEW packages will be installed:\n",
      "  aria2 libaria2-0 libc-ares2 libssh2-1\n",
      "0 upgraded, 4 newly installed, 0 to remove and 8 not upgraded.\n",
      "Need to get 1622 kB of archives.\n",
      "After this operation, 5817 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.2 [45.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libssh2-1 amd64 1.10.0-3 [109 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1086 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
      "Fetched 1622 kB in 3s (620 kB/s)0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libc-ares2:amd64.\n",
      "(Reading database ... 44382 files and directories currently installed.)\n",
      "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libssh2-1:amd64.\n",
      "Preparing to unpack .../libssh2-1_1.10.0-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libssh2-1:amd64 (1.10.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libaria2-0:amd64.\n",
      "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package aria2.\n",
      "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking aria2 (1.36.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libssh2-1:amd64 (1.10.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up aria2 (1.36.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install aria2 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2617d12d-4b6c-48a0-936e-5d7f1d30dbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.34 µs\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "aria2 is already the newest version (1.36.0-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
      "\u001b[35m[\u001b[0m#23e526 141MiB/8.5GiB\u001b[36m(1%)\u001b[0m CN:16 DL:\u001b[32m25MiB\u001b[0m ETA:\u001b[33m5m44s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m^C\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "23e526|\u001b[1;34mINPR\u001b[0m|    25MiB/s|./vicuna-13b-v1.5.ggmlv3.q5_K_M.bin\n",
      "\n",
      "Status Legend:\n",
      "(INPR):download in-progress.\n",
      "\n",
      "aria2 will resume download if the transfer is restarted.\n",
      "If there are any errors, then see the log file. See '-l' option in help/man page for details.\n"
     ]
    }
   ],
   "source": [
    "# モデルのダウンロード\n",
    "%time\n",
    "!apt install -y aria2\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M -d . https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGML/resolve/main/vicuna-13b-v1.5.ggmlv3.q5_K_M.bin -o vicuna-13b-v1.5.ggmlv3.q5_K_M.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56f47cc-61e8-400a-89a3-57fbe41ee8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_MODEL = \"/content/vicuna-13b-v1.5.ggmlv3.q5_K_M.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ea8595-9687-4fe3-b437-4ae9f8b8c793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9013348\n",
      "-rw-rw-r-- 1 1000 1000       1055 Aug 14 01:40 Dockerfile\n",
      "-rw-rw-r-- 1 1000 1000        822 Aug 14 01:41 docker-compose.yml\n",
      "-rw-r--r-- 1 root root      12193 Aug 14 01:56 main.ipynb\n",
      "-rw-r--r-- 1 1000 1000         62 Aug 14 01:54 requirements.txt\n",
      "-rw-r--r-- 1 root root 9229634688 Aug 14 01:56 vicuna-13b-v1.5.ggmlv3.q5_K_M.bin\n",
      "-rw-r--r-- 1 root root       1459 Aug 14 01:56 vicuna-13b-v1.5.ggmlv3.q5_K_M.bin.aria2\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f04e85b-58d2-4667-b69a-543fc04a9069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0d849b-781f-486f-a2a9-67c337b838f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /content/vicuna-13b-v1.5.ggmlv3.q5_K_M.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 6912\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 17 (mostly Q5_K - Medium)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  =  473.53 MB (+  400.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 9455 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# モデルのダウンロード\n",
    "import llama_cpp\n",
    "import ctypes\n",
    "\n",
    "llm =llama_cpp.Llama(model_path=PATH_MODEL, n_gpu_layers=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c13500-2dd0-4d19-80c9-76a41cd8a59c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 基本的な知識とスキルを身につける：プログラミング言語、データ構造、アルゴリズム、ソフトウェア開発の方法論など、プログラマーが必要とされる基本的な知識とスキルを身につけましょう。\n",
      "2. 実際にプロジェクトに参加する：学生時代から友人や仕事でのアルバイターとしてプロジェクトに参加し、実践的なスキルを身につけましょう。\n",
      "3. 継続的に学ぶ：技術は常に進化しているため、継続的に新しい知識や技術を学び、自己成長を重ねましょう。\n",
      "4. コミュニケーション能力を向上させる：チームで作業する場合が多いため、コミュニケーション能力を高め、チームワークを大切にしましょう。\n",
      "5. 自己アピールが重要：自分のスキルや実績をアピールできるようになっておくことが重要です。ポートフォリオやブログ、GitHubなどを活用して、自己アピールを行いましょう。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   222.19 ms\n",
      "llama_print_timings:      sample time =   258.54 ms /   396 runs   (    0.65 ms per token,  1531.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   235.91 ms /    28 tokens (    8.43 ms per token,   118.69 tokens per second)\n",
      "llama_print_timings:        eval time =  7088.70 ms /   395 runs   (   17.95 ms per token,    55.72 tokens per second)\n",
      "llama_print_timings:       total time =  8267.88 ms\n"
     ]
    }
   ],
   "source": [
    "# ref:https://github.com/lm-sys/FastChat/blob/d578599c69d060e6d40943f1b5b72af98956092a/fastchat/conversation.py#L286-L299\n",
    "TEMPLATE = \"\"\"\n",
    "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "全て日本語で返答を行います。\n",
    "\n",
    "USER:Topプログラマーになるにはどうすればいいですか？\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "# 推論の実行\n",
    "output = llm(\n",
    "    TEMPLATE,\n",
    "    max_tokens=3096,\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea5a1c-e069-4582-bef5-e1d56cf9ed23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee94d6-6841-4b49-b41a-e185cb137c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf3f1b94-e2fc-469d-bf85-ca342786a6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/fc/40/ac6dbb262904c7d9efd846360421b334839e0943b869c044096105c573c0/langchain-0.0.263-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.0.263-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Obtaining dependency information for SQLAlchemy<3,>=1.4 from https://files.pythonhosted.org/packages/3a/78/6bb3889e893e4605107a016e63636f278223b0526cb7927323c244ff877c/SQLAlchemy-2.0.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading SQLAlchemy-2.0.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Obtaining dependency information for aiohttp<4.0.0,>=3.8.3 from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Obtaining dependency information for async-timeout<5.0.0,>=4.0.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Obtaining dependency information for dataclasses-json<0.6.0,>=0.5.7 from https://files.pythonhosted.org/packages/97/5f/e7cc90f36152810cab08b6c9c1125e8bcb9d76f8b3018d101b5f877b386c/dataclasses_json-0.5.14-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.11 from https://files.pythonhosted.org/packages/27/ad/adf3554b72b7eb08f3623515bc10dd5e329853e3f0b624fefc715036d358/langsmith-0.0.22-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.0.22-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain)\n",
      "  Obtaining dependency information for numexpr<3.0.0,>=2.8.4 from https://files.pythonhosted.org/packages/57/c7/eb746d20c1a3c85ef6e4743e9e61d1ff8709bea3b28da64100e848c10fdb/numexpr-2.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numexpr-2.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.24.1)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2,>=1 (from langchain)\n",
      "  Obtaining dependency information for pydantic<2,>=1 from https://files.pythonhosted.org/packages/bc/e0/0371e9b6c910afe502e5fe18cc94562bfd9399617c7b4f5b6e13c29115b3/pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.28.1)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.0.263-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Downloading langsmith-0.0.22-py3-none-any.whl (32 kB)\n",
      "Downloading numexpr-2.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, tenacity, pydantic, numexpr, multidict, marshmallow, greenlet, frozenlist, async-timeout, yarl, SQLAlchemy, openapi-schema-pydantic, langsmith, dataclasses-json, aiosignal, aiohttp, langchain\n",
      "Successfully installed SQLAlchemy-2.0.19 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.5.14 frozenlist-1.4.0 greenlet-2.0.2 langchain-0.0.263 langsmith-0.0.22 marshmallow-3.20.1 multidict-6.0.4 numexpr-2.8.5 openapi-schema-pydantic-1.2.4 pydantic-1.10.12 tenacity-8.2.2 typing-inspect-0.9.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6751168-afed-4ada-8d72-35da07805a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55f54ed8-563b-407e-b598-ba06e4b5a701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "はじめまして、今回のエンジニア向けポッドキャストでご紹介するのは、AI技術が建設業界においてもたらす可能性がある効果についてです。最近、建設業界においては、デジタル化が進み、ビッグデータやAIの活用が進んでいます。これにより、建設会社は以前までにないほどの効率的なプロジェクト管理や設計・施工が可能となっており、さらに顧客満足度も向上しています。\n",
      "今回は、建設業界におけるAI技術の活用について、具体的な事例を取り上げながら詳しく説明していきます。まず、AI技術が建設業界でどのような形で使われているかについて見ていきましょう。\n",
      "それでは、最初に取り上げたいのは、プロジェクト管理の分野です。建設会社におけるプロジェクト管理は、多くのデータが発生するため、効率的な管理が求められています。AI技術を活用することで、プロジェクトの進捗状況や品質管理などをより正確に把握することができ、プロジェク\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   222.19 ms\n",
      "llama_print_timings:      sample time =   275.03 ms /   427 runs   (    0.64 ms per token,  1552.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   185.94 ms /    31 tokens (    6.00 ms per token,   166.72 tokens per second)\n",
      "llama_print_timings:        eval time =  7692.95 ms /   426 runs   (   18.06 ms per token,    55.38 tokens per second)\n",
      "llama_print_timings:       total time =  8910.74 ms\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "全て日本語で返答を行います。\n",
    "\n",
    "USER:{user_input}\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(TEMPLATE)\n",
    "formatted_prompt = prompt.format(user_input=\"エンジニア向けポッドキャストの脚本を生成して。\")\n",
    "\n",
    "response = llm(prompt=formatted_prompt, max_tokens=2048)\n",
    "print(response['choices'][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25336fce-33ce-418d-8fab-3262dab254df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a683371-cf08-4a6e-bd4a-86015d415e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5932f61-2845-4104-a839-813cc5ddb19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "PATH_MODEL = \"/content/vicuna-13b-v1.5.ggmlv3.q5_K_M.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b7e017-1718-44a1-9509-96eed46b210a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "全て日本語で返答を行います。\n",
    "\n",
    "USER:次の文章の簡潔な要約をしてください:{text}\n",
    "ASSISTANT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e44bbe5-4fcf-448f-8f23-3495c367cc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1aa01d5-1eea-4e2c-825e-ae1f8d7cf650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from /content/vicuna-13b-v1.5.ggmlv3.q5_K_M.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 6912\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 17 (mostly Q5_K - Medium)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  =  900.73 MB (+ 3200.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 640 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 40/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 9207 MB\n",
      "llama_new_context_with_model: kv self size  = 3200.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=PATH_MODEL,\n",
    "    n_gpu_layers=40,\n",
    "    n_batch=128,\n",
    "    max_tokens=4096,\n",
    "    n_ctx=4096,\n",
    "    # callback_manager=callback_manager, 何故か精度が落ちる\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f236f2f7-f5c9-451e-8006-ac077b6d5687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b7e9dad-e4a2-4ddd-8710-51aca6c19612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[人要らず]ChatGPTを使った一人議論やってますか？ - Qiita\\nsearchsearchLoginSignupsearchTrendQuestionOfficial EventOfficial ColumnOpportunitiesOrganization\\n\\n\\n4947more_horiz@Gamari0009[人要らず]ChatGPTを使った一人議論やってますか？アイデア議論AI個人開発ChatGPTPosted at 2023-08-06\\nはじめに\\n今回は私が良くやっている「一人議論」という方法について紹介いたします。\\nこれはアイデア出しや壁打ちに便利な手法です。\\n「使えるかも」と思った方は、ぜひやってみてください。\\n\\n一人議論とは？\\nChatGPTを使った議論のシミュレーションの一つです。\\n具体的には次のようなプロンプトを利用して行います。\\n# タスク\\nテーマについて議論してください。\\n\\n# ルール\\n- 登場人物を3人出してください。\\n- 議論をまとめないでください。\\n- 10回会話を続けたところで会話を止めてください。\\n\\n# テーマ\\nChatGPTがもたらしたもの\\n\\nすると、次のような議論が始まります。\\n登場人物\\n\\n佐藤 - 30代、IT系企業で働く男性。ChatGPTを日常的に業務や学習のサポートとして使っている。\\n美香 - 40代、教育者。中学校の先生として勤務。ChatGPTとの教育現場での適用に懐疑的。\\nハルト - 20代、大学生。ChatGPTの技術や社会的影響に深く興味を持っている。\\n佐藤: ChatGPTのおかげで、仕事が格段に効率的になったよ。特に情報収集や書類作成の際のアシスタントとしての役割は大きい。\\n\\n美香: でも、教育の現場から見ると少し心配な面もあるんだよね。生徒たちが自ら考えることをやめて、ChatGPTに頼りきりになることが増えてきている気がする。\\n\\nハルト: 確かに、依存の度合いが強まると問題だよね。でも、それはChatGPTのせいというより、どう使うか、どう教育するかの問題じゃない？\\n\\n佐藤: その点は同意だね。ツールとしての使い方を間違えなければ、非常に有益なものだと思う。\\n\\n美香: でも、心の中での問答や疑問を外部のツールに頼ることで、人間の独自の思考力や創造力が失われるのではないかとも思うの。\\n\\nハルト: その点も確かに考えられるけど、例えば計算機が普及したときも同じような議論があったよね。計算能力が落ちるとか、頭を使わなくなるとか。\\n\\n佐藤: そうだね。でも、計算機のおかげで、より複雑な問題に取り組めるようになったとも言える。ChatGPTも、私たちの問題解決の幅を広げてくれるツールとして捉えることができるんじゃないかな。\\n\\n美香: それはそうかもしれないけど、子供たちがこの技術に触れる年齢がどんどん下がっているのも事実。思考の発達段階で外部の助けを過度に頼るのは、成長を妨げる要因になるのでは？\\n\\nハルト: それは大事なポイントだね。技術の普及と教育の方針、これからどうバランスをとるかが鍵になってくるんじゃないかな。\\n\\n佐藤: 確かに、技術の進化とともに、それをどう育てるか、どう教えるか、その方法も見直していく必要があるよね。\\n\\nここからが一人議論のポイントです。\\n次に、自分の意見を差し込みます。\\nChatGPTにて次のような文章を打ち込みます。\\n\\nすると、それを元に、次のような会話が続きます。\\n**ハルト:** 良いトピックだね。エンジニアの世界では、ChatGPTを始めとするAI技術の進化により、コードの生成やバグの修正など、多くのタスクが自動化される方向に進んでるよね。\\n\\n**佐藤:** ああ、そうだね。例えば、ChatGPTのようなモデルがコードの提案や最適化の助言をしてくれるので、初心者エンジニアの学習カーブが緩やかになったと感じるよ。\\n\\n**美香:** でも、それってエンジニアのスキルや役割を減少させるのでは？全てのことがAIに依存するようになったら、人間のエンジニアは不要になるの？\\n\\n**ハルト:** 確かに一部のルーチンワークは自動化されるかもしれないけど、人間の直感や創造性を持ったAIはまだ実現されてないよ。エンジニアの役割は変わるかもしれないけど、なくなることはないと思う。\\n\\n**佐藤:** それに、新しい技術の台頭は新しい役割やスキルを生む可能性もあるよ。例えば、AIモデルの調整やカスタマイズ、エシカルな問題の解決など、これからのエンジニアが取り組むべき課題も増えると思う。\\n\\n**美香:** そういう視点から言えば、エンジニアはただのコーダーから、より深く社会やビジネスの問題を解決する役割へとシフトしていくのかな。\\n\\n**ハルト:** そうだね。技術の進化は、エンジニアにとっても新しいチャンスを生む。ただ、そのチャンスを活かすためには、常に学び続ける姿勢が必要だろうね。\\n\\n**佐藤:** 確かに、技術の進化は止まらない。でも、それがエンジニアの可能性を広げるものと捉えることが大切だと思う。\\n\\n**美香:** つまり、ChatGPTがもたらす変化を恐れるのではなく、どう活用するか、どう適応するかの視点が重要ってことね。\\n\\n**ハルト:** まさにその通り。エンジニアの役割は変わっても、その背後にある問題解決や価値提供の精神は変わらないはずだから。\\n\\nあとはひたすらこれを続けるだけです。\\n強めの反論をしても誰も怒りませんし、自分の話したいテーマを選択できるので非常に便利です。\\n\\nまとめ\\n今回は一人議論について説明させていただきました。\\n今後は、何か議論したいときはChatGPTを使って議論することが可能ですね！\\n\\n紹介\\nその他にもこんな記事を書いています。\\nよかったら見ていってください。\\n\\n\\n\\n\\n\\n\\nRegister as a new user and use Qiita more convenientlyYou get articles that match your needsYou can efficiently read back useful informationWhat you can do with signing upSign upLogin4947more_horiz\\n\\nHow developers code is here.© 2011-2023Qiita Inc.Guide & HelpAboutTermsPrivacyGuidelineDesign GuidelineFeedbackHelpAdvertisementContentsRelease NoteOfficial EventOfficial ColumnOpportunitiesAdvent CalendarQiita AwardAPISNS@Qiita@qiita_milestone@qiitapoi@QiitaOur serviceQiita TeamQiita JobsQiita ZineOfficial ShopCompanyAbout UsCareersQiita Blog\\n\\n\\n\\n', metadata={'source': 'https://qiita.com/Gamari0009/items/37a5b7b69ce9dfdb670e', 'title': '[人要らず]ChatGPTを使った一人議論やってますか？ - Qiita', 'description': 'はじめに今回は私が良くやっている「一人議論」という方法について紹介いたします。これはアイデア出しや壁打ちに便利な手法です。「使えるかも」と思った方は、ぜひやってみてください。一人議論とは？…', 'language': 'ja'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"\"\"\n",
    "https://qiita.com/Gamari0009/items/37a5b7b69ce9dfdb670e\n",
    "\"\"\"\n",
    "loader = WebBaseLoader(url.replace(\"\\n\", \"\"))\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ec4e159-543b-47e1-861b-322d1b67fe34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人要らず]ChatGPTを使った一人論については、佐藤、美香、ハルトの3人の登場人物が参加して、テーマである「ChatGPTがもたらしたもの」について論しています。会話は10回間けられます。佐藤は事の化に喜びを持っていますが、美香は教育現場での用に的であり、ハルトは技術や社会的影にを持っています。3人はそれれの意見を交わしながら、チャットGPTの利用方法や技術の進化によるエンジニアの変化について話し合っています。最後には、一人論を通じて自分の意見をしむことができると言及されています。\n",
      "---\n",
      "メタディスクラシスとは何か？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   573.30 ms\n",
      "llama_print_timings:      sample time =   218.23 ms /   309 runs   (    0.71 ms per token,  1415.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21346.73 ms /  2745 tokens (    7.78 ms per token,   128.59 tokens per second)\n",
      "llama_print_timings:        eval time = 53873.98 ms /   308 runs   (  174.92 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time = 76122.40 ms\n"
     ]
    }
   ],
   "source": [
    "print(stuff_chain.run(docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
