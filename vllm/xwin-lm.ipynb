{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8fbd3-c9c6-4949-8457-34a3c084f184",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|█████| 675/675 [00:00<00:00, 7.33MB/s]\n",
      "INFO 09-22 07:05:45 llm_engine.py:72] Initializing an LLM engine with config: model='TheBloke/Xwin-LM-13B-V0.1-AWQ', tokenizer='TheBloke/Xwin-LM-13B-V0.1-AWQ', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.float16, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 748/748 [00:00<00:00, 6.66MB/s]\n",
      "Downloading tokenizer.model: 100%|████████████| 500k/500k [00:00<00:00, 711kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|█| 1.84M/1.84M [00:00<00:00, 2.56MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████| 438/438 [00:00<00:00, 4.83MB/s]\n",
      "Downloading (…)e6/quant_config.json: 100%|████| 90.0/90.0 [00:00<00:00, 970kB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████| 183/183 [00:00<00:00, 2.60MB/s]\n",
      "Downloading model.safetensors: 100%|███████| 7.25G/7.25G [05:19<00:00, 22.7MB/s]\n",
      "INFO 09-22 07:11:17 llm_engine.py:202] # GPU blocks: 1074, # CPU blocks: 327\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m80\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 09-22 07:11:33 async_llm_engine.py:371] Received request 61a7080114e348b29d8cae2138b4163b: prompt: '富士山について教えて', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=256, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:11:33 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:11:35 async_llm_engine.py:111] Finished request 61a7080114e348b29d8cae2138b4163b.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:42664 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:12:01 async_llm_engine.py:371] Received request 372a694a1c6c42f7a77a0666f89d4561: prompt: '富士山についてわかっていることを教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=256, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:12:01 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:12:05 async_llm_engine.py:111] Finished request 372a694a1c6c42f7a77a0666f89d4561.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:36724 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:12:21 async_llm_engine.py:371] Received request c8419226912e4dccba9a3b9066d8a14c: prompt: '富士山についてわかっていることを教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:12:21 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:12:26 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:12:28 async_llm_engine.py:111] Finished request c8419226912e4dccba9a3b9066d8a14c.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:60602 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:13:08 async_llm_engine.py:371] Received request cad71fb3284e44bab5ad5b2279f8bb03: prompt: '富士山の高さは?', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:13:08 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:13:13 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:13:14 async_llm_engine.py:111] Finished request cad71fb3284e44bab5ad5b2279f8bb03.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:54730 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:13:37 async_llm_engine.py:371] Received request 6af6932799344106acf0d9c6efdbcbaf: prompt: '富士山の高さは?', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:13:38 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:13:38 async_llm_engine.py:111] Finished request 6af6932799344106acf0d9c6efdbcbaf.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:59064 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:13:49 async_llm_engine.py:371] Received request 07f39a05c33d4b1ca543c6e57842d7c3: prompt: '富士山の高さは?', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:13:49 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:13:54 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:13:57 async_llm_engine.py:111] Finished request 07f39a05c33d4b1ca543c6e57842d7c3.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:42862 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:14:18 async_llm_engine.py:371] Received request 051af7d19aba41909f81b26dcf5bb466: prompt: '富士山の高さは?', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:14:18 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:14:18 async_llm_engine.py:111] Finished request 051af7d19aba41909f81b26dcf5bb466.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:37044 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:14:20 async_llm_engine.py:371] Received request 4d2c1d9dbcd841a48aff337739a75d18: prompt: '富士山の高さは?', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:14:23 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:14:28 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:14:28 async_llm_engine.py:111] Finished request 4d2c1d9dbcd841a48aff337739a75d18.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:37052 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:14:41 async_llm_engine.py:371] Received request f9fd828c20764f21b42e5a222f2201e9: prompt: '日本の総理大臣は？', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:14:41 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:14:46 async_llm_engine.py:111] Finished request f9fd828c20764f21b42e5a222f2201e9.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:52234 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:15:43 async_llm_engine.py:371] Received request 00648895ff86471fa5d7501b3e5ed001: prompt: '日本の総理大臣は？', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:15:43 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:15:44 async_llm_engine.py:111] Finished request 00648895ff86471fa5d7501b3e5ed001.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:40458 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:15:47 async_llm_engine.py:371] Received request 8ada4c66b1b64b5f9eedeb9ef0e7312f: prompt: '日本の総理大臣は？', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:15:48 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:15:48 async_llm_engine.py:111] Finished request 8ada4c66b1b64b5f9eedeb9ef0e7312f.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:40706 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:16:05 async_llm_engine.py:371] Received request cb394e330199499289ee7f74a14194d0: prompt: '富士山の高さは？？', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:16:05 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:16:06 async_llm_engine.py:111] Finished request cb394e330199499289ee7f74a14194d0.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:38228 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:16:08 async_llm_engine.py:371] Received request f7632019818f442c9cc959ed1aff6163: prompt: '富士山の高さは？？', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:16:09 async_llm_engine.py:111] Finished request f7632019818f442c9cc959ed1aff6163.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:56408 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:17:15 async_llm_engine.py:371] Received request 255ae88e27a54972b099475fecbfeb6b: prompt: 'Twitterアプリの画面遷移図をPlantUMLで出力して', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:17:15 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:17:20 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:17:21 async_llm_engine.py:111] Finished request 255ae88e27a54972b099475fecbfeb6b.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:59934 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:17:46 async_llm_engine.py:371] Received request 9688656323634950979ae93c6b1728ab: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:17:46 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:17:51 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:17:54 async_llm_engine.py:111] Finished request 9688656323634950979ae93c6b1728ab.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:47306 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:17:59 async_llm_engine.py:371] Received request 13ce2e48a4ba4567a46709cc421bc600: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:17:59 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:18:04 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:18:08 async_llm_engine.py:111] Finished request 13ce2e48a4ba4567a46709cc421bc600.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:44142 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:18:16 async_llm_engine.py:371] Received request 5c3d7e47e53847d0949c96a18ca7d8ce: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:18:16 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:18:21 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:18:24 async_llm_engine.py:111] Finished request 5c3d7e47e53847d0949c96a18ca7d8ce.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:47654 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:18:32 async_llm_engine.py:371] Received request 289b8155fa404527959e93b71fe31a1b: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:18:32 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:18:37 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:18:40 async_llm_engine.py:111] Finished request 289b8155fa404527959e93b71fe31a1b.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:53544 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:18:45 async_llm_engine.py:371] Received request 76337def8fd44c03a5ad53fe666dbb56: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:18:46 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:18:51 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:18:54 async_llm_engine.py:111] Finished request 76337def8fd44c03a5ad53fe666dbb56.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:49918 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:19:00 async_llm_engine.py:371] Received request b91bd57c9101440ea2305018504c7f4e: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:19:01 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:19:06 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:19:09 async_llm_engine.py:111] Finished request b91bd57c9101440ea2305018504c7f4e.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:38140 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:19:14 async_llm_engine.py:371] Received request de96737896ca4980ad01e015cf26c9be: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:19:14 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:19:19 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:19:22 async_llm_engine.py:111] Finished request de96737896ca4980ad01e015cf26c9be.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:43650 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:19:25 async_llm_engine.py:371] Received request d71f9f539665423b9c50f8c5a17be534: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:19:25 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:19:30 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:19:33 async_llm_engine.py:111] Finished request d71f9f539665423b9c50f8c5a17be534.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:38846 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:19:39 async_llm_engine.py:371] Received request f1d8198bcf284024830d6c4d85c23870: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:19:39 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:19:44 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:19:47 async_llm_engine.py:111] Finished request f1d8198bcf284024830d6c4d85c23870.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:46142 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:21:29 async_llm_engine.py:371] Received request 3a949fabf22b4cc490715057eb0bdbf9: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:21:29 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:21:34 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:21:37 async_llm_engine.py:111] Finished request 3a949fabf22b4cc490715057eb0bdbf9.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:37898 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:21:44 async_llm_engine.py:371] Received request e244442631824df7b7521b6afc5e8481: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.3, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:21:44 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:21:47 async_llm_engine.py:111] Finished request e244442631824df7b7521b6afc5e8481.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:58670 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:22:03 async_llm_engine.py:371] Received request 9ad3898e2bc04bc98277074491cd5bcf: prompt: 'カレーの作り方を教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.3, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:22:03 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:22:08 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:22:11 async_llm_engine.py:111] Finished request 9ad3898e2bc04bc98277074491cd5bcf.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:37328 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:22:37 async_llm_engine.py:371] Received request cd24d0468924482bb7842927eca6cfca: prompt: '味噌の作り方', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.3, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:22:37 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:22:42 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:22:45 async_llm_engine.py:111] Finished request cd24d0468924482bb7842927eca6cfca.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:39562 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:24:16 async_llm_engine.py:371] Received request 4d94c00f68af45a285d56dee4fced343: prompt: '医師による鼻血の止め方。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.3, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:24:17 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:24:22 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:24:25 async_llm_engine.py:111] Finished request 4d94c00f68af45a285d56dee4fced343.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:59404 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 09-22 07:27:33 async_llm_engine.py:371] Received request c005bd27328341eeadb9c86315eb3840: prompt: 'JSのフレームワークのRemixについて知っていることを教えて。', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.3, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=512, logprobs=None), prompt token ids: None.\n",
      "INFO 09-22 07:27:33 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:27:38 llm_engine.py:613] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%\n",
      "INFO 09-22 07:27:42 async_llm_engine.py:111] Finished request c005bd27328341eeadb9c86315eb3840.\n",
      "\u001b[32mINFO\u001b[0m:     172.19.0.1:36954 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m vllm.entrypoints.api_server --host \"0.0.0.0\" --model TheBloke/Xwin-LM-13B-V0.1-AWQ --quantization awq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934e097e-05af-4c8d-ae82-1cca550631ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955dfdd074f54627b8711ed78968acea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100% 40/40 [00:03<00:00, 10.92it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"TheBloke/Xwin-LM-13B-V0.1-AWQ\" # or 7B\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, # fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "\n",
    "def chat(prompt, max_new_tokens):\n",
    "    tokens = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt'\n",
    "    ).input_ids.cuda()\n",
    "\n",
    "    # Generate output\n",
    "    generation_output = model.generate(\n",
    "        tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    return tokenizer.decode(generation_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fdc82be-59bd-45cf-be4c-d68300529427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 日本の観光地の平均気温をjsonで書き出すと、以下のようなコードがあります。\n",
      "```javascript\n",
      "const axios = require('axios');\n",
      "\n",
      "const url = 'https://api.ipapi.co/2/json/24/all/weather/sunshine/japan/';\n",
      "const filePath = 'japan_sunshine.json';\n",
      "\n",
      "axios.get(url)\n",
      "  .then(response => {\n",
      "    const data = response.data.data.list;\n",
      "    const output = data.map(item => {\n",
      "      const temp = item.main.temp_max;\n",
      "      const sunshine = item.weather[0].sun.sunshine;\n",
      "      const weather = item.weather[0].main;\n",
      "      return {\n",
      "        city: item.city.name_ja,\n",
      "        temperature: `${temp}℃`,\n",
      "        sunshine: `${sunshine}%`,\n",
      "        weather: `${weather}`,\n",
      "      };\n",
      "    });\n",
      "    fs.writeFileSync(filePath, JSON.stringify(output, null, 2));\n",
      "    console.log('Data saved to',\n",
      "CPU times: user 4.09 s, sys: 15.7 ms, total: 4.1 s\n",
      "Wall time: 4.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"\"\"日本の観光地の平均気温をjsonで書き出すと、\"\"\"\n",
    "print(chat(prompt, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0d174b-b047-442f-8692-b4958bc528b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 小学生向けに計算問題(2桁の四則演算)を5問出力してください。まず問題を並べて、そのあとに解答のみを並べてください。\n",
      "\n",
      "問題:\n",
      "1. 1 + 2 = ?\n",
      "2. 3 - 4 = ?\n",
      "3. 6 \\* 2 = ?\n",
      "4. 9 / 3 = ?\n",
      "5. 4 \\* 5 = ?\n",
      "\n",
      "解答:\n",
      "1. 3\n",
      "2. -1\n",
      "3. 12\n",
      "4. 3\n",
      "5. 20</s>\n",
      "CPU times: user 1.58 s, sys: 1.15 ms, total: 1.58 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"\"\"小学生向けに計算問題(2桁の四則演算)を5問出力してください。まず問題を並べて、そのあとに解答のみを並べてください。\n",
    "\n",
    "\"\"\"\n",
    "print(chat(prompt, 256))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
